{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b555959c",
   "metadata": {},
   "source": [
    "# LLM-Human Agreement: SHAP Narrative Evaluation (Krippendorff's Alpha)\n",
    "\n",
    "Computes Krippendorff's Alpha to measure agreement between human and LLM judges on ML SHAP explanation narratives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "541de858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import krippendorff\n",
    "\n",
    "# --- Configuration ---\n",
    "NUM_NARRATIVES = 60\n",
    "VALUE_DOMAIN = [1, 2, 3, 4, 5]\n",
    "METRIC = 'interval'\n",
    "\n",
    "# --- Load LLM Judcge Data ---\n",
    "llm_data = pd.read_csv('../../data/output/llm_judge_evaluation_results.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3633ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate Dummy Human Scores ---\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Generate human scores with some correlation to LLM scores but with variation\n",
    "human_scores_faithfulness = np.random.randint(1, 6, NUM_NARRATIVES)\n",
    "human_scores_completeness = np.random.randint(1, 6, NUM_NARRATIVES)\n",
    "human_scores_clarity = np.random.randint(1, 6, NUM_NARRATIVES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f50a651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract LLM scores for the three metrics\n",
    "import ast\n",
    "\n",
    "# Convert string representations of dicts to actual dicts\n",
    "llm_data['evaluation'] = llm_data['evaluation'].apply(ast.literal_eval)\n",
    "\n",
    "# Now extract scores\n",
    "llm_scores_faithfulness = llm_data['evaluation'].apply(lambda x: x['factual_accuracy']['score']).values\n",
    "llm_scores_completeness = llm_data['evaluation'].apply(lambda x: x['completeness_integration']['score']).values\n",
    "llm_scores_clarity = llm_data['evaluation'].apply(lambda x: x['clarity_readability']['score']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ba8dd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_scores_faithfulness: [4 5 3 5 5 2 3 3 3 5 4 3 5 2 4 2 4 5 1 4 2 5 4 1 1 3 3 2 4 4 3 4 4 1 3 5 3\n",
      " 5 1 2 4 1 4 2 2 1 2 5 2 4 4 4 4 5 3 1 4 2 4 2]\n",
      "\n",
      "human_scores_completeness [2 4 5 2 2 4 2 2 4 4 1 5 5 2 5 2 1 4 4 4 5 1 5 5 1 1 1 1 4 3 3 1 3 3 1 3 5\n",
      " 2 2 1 4 1 4 2 1 5 3 4 3 3 1 3 5 3 1 5 2 3 1 2]\n",
      "\n",
      "human_scores_clarity [2 4 5 3 1 4 5 4 5 5 3 5 4 5 3 3 4 2 2 5 1 5 4 4 4 4 4 3 2 4 1 1 1 1 3 1 4\n",
      " 5 1 3 3 1 5 1 3 2 4 3 1 4 1 1 2 4 4 2 3 1 5 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"human_scores_faithfulness:\", human_scores_faithfulness)\n",
    "print(\"\\nhuman_scores_completeness\", human_scores_completeness)\n",
    "print(\"\\nhuman_scores_clarity\", human_scores_clarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04ab343f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm_scores_faithfulness: [3 3 3 3 3 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 1 1 3 3 1 1 1 3 1 3 3 1 3 1\n",
      " 3 3 3 3 1 1 3 1 3 3 3 3 1 3 1 1 1 1 3 3 3 3 1]\n",
      "\n",
      "llm_scores_completeness [3 3 3 3 3 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 2 2 3 3 2 2 2 3 2 3 3 2 3 2\n",
      " 3 3 3 3 2 2 3 2 2 3 3 3 1 3 2 2 2 2 3 3 3 3 2]\n",
      "\n",
      "llm_scores_clarity [4 4 4 4 4 3 4 3 4 4 4 4 4 4 4 4 4 4 4 4 4 3 4 3 3 4 4 3 3 3 4 4 4 4 3 4 3\n",
      " 4 4 4 4 3 3 4 3 4 4 4 4 3 4 3 3 3 3 4 4 4 4 3]\n"
     ]
    }
   ],
   "source": [
    "print(\"llm_scores_faithfulness:\", llm_scores_faithfulness)\n",
    "print(\"\\nllm_scores_completeness\", llm_scores_completeness)\n",
    "print(\"\\nllm_scores_clarity\", llm_scores_clarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed85fce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Faithfulness ---\n",
      "Krippendorff's Alpha for Faithfulness: -0.054\n",
      "\n",
      "--- Completeness ---\n",
      "Krippendorff's Alpha for Completeness: 0.060\n",
      "\n",
      "--- Clarity ---\n",
      "Krippendorff's Alpha for Clarity: -0.139\n",
      "\n",
      "--- Interpretation ---\n",
      "Krippendorff's Alpha values typically range from -1 to 1:\n",
      "  1.0   = Perfect agreement\n",
      "  0.80+ = Excellent agreement\n",
      "  0.67+ = Good agreement (often considered acceptable for drawing tentative conclusions)\n",
      "  <0.67 = Questionable agreement (requires careful consideration)\n",
      "  0.0   = Agreement expected by chance\n",
      "  <0.0  = Systematic disagreement\n",
      "\n",
      "Your alpha values indicate the level of consistency between your human judge and LLM judge for each aspect.\n",
      "A higher alpha value suggests better alignment between their scores.\n"
     ]
    }
   ],
   "source": [
    "# --- Prepare Reliability Data Arrays ---\n",
    "reliability_data_faithfulness = np.array([human_scores_faithfulness, llm_scores_faithfulness])\n",
    "reliability_data_completeness = np.array([human_scores_completeness, llm_scores_completeness])\n",
    "reliability_data_clarity = np.array([human_scores_clarity, llm_scores_clarity])\n",
    "\n",
    "# --- Calculate Krippendorff's Alpha for each metric ---\n",
    "\n",
    "print(\"--- Faithfulness ---\")\n",
    "alpha_faithfulness = krippendorff.alpha(reliability_data=reliability_data_faithfulness,\n",
    "                                       value_domain=VALUE_DOMAIN)\n",
    "print(f\"Krippendorff's Alpha for Faithfulness: {alpha_faithfulness:.3f}\")\n",
    "\n",
    "print(\"\\n--- Completeness ---\")\n",
    "alpha_completeness = krippendorff.alpha(reliability_data=reliability_data_completeness,\n",
    "                                       value_domain=VALUE_DOMAIN)\n",
    "print(f\"Krippendorff's Alpha for Completeness: {alpha_completeness:.3f}\")\n",
    "\n",
    "print(\"\\n--- Clarity ---\")\n",
    "alpha_clarity = krippendorff.alpha(reliability_data=reliability_data_clarity,\n",
    "                                  value_domain=VALUE_DOMAIN)\n",
    "print(f\"Krippendorff's Alpha for Clarity: {alpha_clarity:.3f}\")\n",
    "\n",
    "# --- Interpretation Guidance ---\n",
    "print(\"\\n--- Interpretation ---\")\n",
    "print(\"Krippendorff's Alpha values typically range from -1 to 1:\")\n",
    "print(\"  1.0   = Perfect agreement\")\n",
    "print(\"  0.80+ = Excellent agreement\")\n",
    "print(\"  0.67+ = Good agreement (often considered acceptable for drawing tentative conclusions)\")\n",
    "print(\"  <0.67 = Questionable agreement (requires careful consideration)\")\n",
    "print(\"  0.0   = Agreement expected by chance\")\n",
    "print(\"  <0.0  = Systematic disagreement\")\n",
    "print(\"\\nYour alpha values indicate the level of consistency between your human judge and LLM judge for each aspect.\")\n",
    "print(\"A higher alpha value suggests better alignment between their scores.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
