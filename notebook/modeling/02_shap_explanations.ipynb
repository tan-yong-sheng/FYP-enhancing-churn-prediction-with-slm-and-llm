{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a69a63dc",
   "metadata": {},
   "source": [
    "# Evaluation on LLM-generated narratives for SHAP explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1355dc84",
   "metadata": {},
   "source": [
    "## Part 1: Data loading & Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70753b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the cleaned data\n",
    "import pandas as pd\n",
    "\n",
    "merged_data_final = pd.read_csv(\"../../data/processed/cleaned_data.csv.gz\", compression=\"gzip\")\n",
    "\n",
    "X = merged_data_final.drop(['churn_risk_score'] ,axis = 1)\n",
    "y = merged_data_final['churn_risk_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce49a7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_data_final.drop(['churn_risk_score'] ,axis = 1)\n",
    "y = merged_data_final['churn_risk_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f4870e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "                    X, y, train_size=0.6, \n",
    "                    stratify= y,\n",
    "                    random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "                    X_temp, y_temp, train_size=0.5,\n",
    "                    stratify=y_temp,\n",
    "                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92405feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export train and test dataset to `data` folder\n",
    "pd.concat([X_train, y_train], axis=1).to_csv(\"../../data/input/train.csv.gz\", index=False)\n",
    "pd.concat([X_valid, y_valid], axis=1).to_csv(\"../../data/input/valid.csv.gz\", index=False)\n",
    "pd.concat([X_test, y_test], axis=1).to_csv(\"../../data/input/test.csv.gz\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c23477b",
   "metadata": {},
   "source": [
    "### Text Representation with LLM embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a07a4dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "from utils.prepare_llm_embedding import generate_embeddings_from_series \n",
    "\n",
    "EMBEDDING_TRAIN = \"../../data/processed/llm_embedding_train.csv.gz\"\n",
    "if os.path.exists(EMBEDDING_TRAIN):\n",
    "    pass\n",
    "else:\n",
    "    processed_text_series = pd.Series(X_train[\"feedback\"].to_list(),\n",
    "                                    index=X_train['id'].to_list()) \n",
    "    llm_embedding_train = generate_embeddings_from_series(processed_text_series,\n",
    "                            additional_data={\"churn_risk_score\": y_train.to_list()},\n",
    "                            output_csv_path=\"../../data/processed/llm_embedding_train.csv.gz\",\n",
    "                            max_workers=20)\n",
    "    print(llm_embedding_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b37d21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "from utils.prepare_llm_embedding import generate_embeddings_from_series \n",
    "\n",
    "EMBEDDING_VALID = \"../../data/processed/llm_embedding_valid.csv.gz\"\n",
    "if os.path.exists(EMBEDDING_VALID):\n",
    "    pass\n",
    "else:\n",
    "    processed_text_series = pd.Series(X_valid[\"feedback\"].to_list(),\n",
    "                                    index=X_valid['id'].to_list()) \n",
    "    llm_embedding_valid = generate_embeddings_from_series(processed_text_series,\n",
    "                            additional_data={\"churn_risk_score\": y_valid.to_list()},\n",
    "                            output_csv_path=\"../../data/processed/llm_embedding_valid.csv.gz\",\n",
    "                            max_workers=20)\n",
    "    print(llm_embedding_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80b39d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "from utils.prepare_llm_embedding import generate_embeddings_from_series \n",
    "\n",
    "EMBEDDING_TEST = \"../../data/processed/llm_embedding_test.csv.gz\"\n",
    "if os.path.exists(EMBEDDING_TEST):\n",
    "    pass\n",
    "else:\n",
    "    processed_text_series = pd.Series(X_test[\"feedback\"].to_list(),\n",
    "                                    index=X_test['id'].to_list()) \n",
    "    llm_embedding_test= generate_embeddings_from_series(processed_text_series,\n",
    "                            additional_data={\"churn_risk_score\": y_test.to_list()},\n",
    "                            output_csv_path=\"../../data/processed/llm_embedding_test.csv.gz\",\n",
    "                            max_workers=20)\n",
    "    print(llm_embedding_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414654d8",
   "metadata": {},
   "source": [
    "### Combine LLM embeddings with structured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e421379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_vectorized = pd.read_csv(\"../../data/processed/llm_embedding_train.csv.gz\", compression=\"gzip\")\n",
    "valid_text_vectorized = pd.read_csv(\"../../data/processed/llm_embedding_valid.csv.gz\", compression=\"gzip\")\n",
    "test_text_vectorized = pd.read_csv(\"../../data/processed/llm_embedding_test.csv.gz\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49807123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Create train df which combines all structured data and textual data\n",
    "train_df = pd.merge(X_train, train_text_vectorized[['id', 'embedding_json', 'churn_risk_score']], on='id', how='outer')\n",
    "train_df['embedding_json'] = train_df['embedding_json'].apply(ast.literal_eval)\n",
    "\n",
    "# create dataframe with columns with 'unstructured data'\n",
    "X_train_unstructured_llm_embedding = pd.DataFrame(train_df['embedding_json'].tolist())\n",
    "X_train_unstructured_llm_embedding.columns = [f\"text_feature_{i+1}\" for i in range(X_train_unstructured_llm_embedding.shape[1])]\n",
    "\n",
    "# create dataframe with columns with 'structured data'\n",
    "X_train_structured = train_df.drop(columns=['feedback', 'embedding_json', 'churn_risk_score'])\n",
    "\n",
    "# Concatenate with both dataframe with structured and unstructured data\n",
    "X_train_vectorized = pd.concat([X_train_structured, X_train_unstructured_llm_embedding], axis=1).drop(columns=['id'])\n",
    "X_train_vectorized_with_id = pd.concat([X_train_structured, X_train_unstructured_llm_embedding], axis=1)\n",
    "\n",
    "# Create target variable\n",
    "y_train = train_df['churn_risk_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8f0bb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation df which combines all structured data and textual data\n",
    "valid_df = pd.merge(X_valid, valid_text_vectorized[['id', 'embedding_json', 'churn_risk_score']], on='id', how='outer')\n",
    "valid_df['embedding_json'] = valid_df['embedding_json'].apply(ast.literal_eval)\n",
    "\n",
    "# create dataframe with columns with 'unstructured data'\n",
    "X_valid_unstructured_llm_embedding = pd.DataFrame(valid_df['embedding_json'].tolist())\n",
    "X_valid_unstructured_llm_embedding.columns = [f\"text_feature_{i+1}\" for i in range(X_valid_unstructured_llm_embedding.shape[1])]\n",
    "\n",
    "# create dataframe with columns with 'structured data'\n",
    "X_valid_structured = valid_df.drop(columns=['feedback', 'embedding_json', 'churn_risk_score'])\n",
    "\n",
    "# Concatenate with both dataframe with structured and unstructured data\n",
    "X_valid_vectorized  = pd.concat([X_valid_structured, X_valid_unstructured_llm_embedding], axis=1).drop(columns=['id'])\n",
    "X_valid_vectorized_with_id  = pd.concat([X_valid_structured, X_valid_unstructured_llm_embedding], axis=1)\n",
    "\n",
    "# Create target variable\n",
    "y_valid = valid_df['churn_risk_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dab5558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test df which combines all structured data and textual data\n",
    "test_df = pd.merge(X_test, test_text_vectorized[['id', 'embedding_json', 'churn_risk_score']], on='id', how='outer')\n",
    "test_df['embedding_json'] = test_df['embedding_json'].apply(ast.literal_eval)\n",
    "\n",
    "# create dataframe with columns with 'unstructured data'\n",
    "X_test_unstructured_llm_embedding = pd.DataFrame(test_df['embedding_json'].tolist())\n",
    "X_test_unstructured_llm_embedding.columns = [f\"text_feature_{i+1}\" for i in range(X_test_unstructured_llm_embedding.shape[1])]\n",
    "\n",
    "# create dataframe with columns with 'structured data'\n",
    "X_test_structured = test_df.drop(columns=['feedback', 'embedding_json', 'churn_risk_score'])\n",
    "\n",
    "## Concatenate with original dataframe (drop embedding_json)\n",
    "X_test_vectorized = pd.concat([X_test_structured, X_test_unstructured_llm_embedding], axis=1).drop(columns=['id'])\n",
    "X_test_vectorized_with_id = pd.concat([X_test_structured, X_test_unstructured_llm_embedding], axis=1)\n",
    "\n",
    "# Create target variable\n",
    "y_test = test_df['churn_risk_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731b5ea4",
   "metadata": {},
   "source": [
    "### Pick the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "821c6e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 1.0000\n",
      "Valid Accuracy:  0.9278\n",
      "Test Accuracy:  0.9284\n",
      "\n",
      "Train F1-score: 1.0000\n",
      "Valid F1-score:  0.9276\n",
      "Test F1-score:  0.9282\n",
      "\n",
      "Train Precision: 1.0000\n",
      "Valid Precision:  0.9286\n",
      "Test Precision:  0.9287\n",
      "\n",
      "Train Recall: 1.0000\n",
      "Valid Recall:  0.9278\n",
      "Test Recall:  0.9284\n",
      "\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.90      0.92      3396\n",
      "           1       0.92      0.95      0.93      4003\n",
      "\n",
      "    accuracy                           0.93      7399\n",
      "   macro avg       0.93      0.93      0.93      7399\n",
      "weighted avg       0.93      0.93      0.93      7399\n",
      "\n",
      "Confusion Matrix (Test):\n",
      "[[3068  328]\n",
      " [ 202 3801]]\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import (f1_score, accuracy_score, precision_score, \n",
    "                            recall_score, classification_report, confusion_matrix)\n",
    "\n",
    "# Train the model\n",
    "xgb_model = xgb.XGBClassifier(max_depth=10,\n",
    "                            random_state=42,\n",
    "                            # Introduce randomness to make training faster and reduce overfitting\n",
    "                            subsample=0.8, ## Uses 80% of the data for each tree.\n",
    "                            colsample_bytree=0.8, ## Uses 80% of the features for each tree.\n",
    "                            # the parameters below make the model trained faster by enabling parallelism\n",
    "                            n_jobs = -1)\n",
    "xgb_model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predictions on training and test sets\n",
    "y_train_pred_xgb = xgb_model.predict(X_train_vectorized)\n",
    "y_valid_pred_xgb = xgb_model.predict(X_valid_vectorized)\n",
    "y_test_pred_xgb = xgb_model.predict(X_test_vectorized)\n",
    "\n",
    "# Accuracy scores\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred_xgb)\n",
    "valid_accuracy = accuracy_score(y_valid, y_valid_pred_xgb)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred_xgb)\n",
    "\n",
    "# F1 scores\n",
    "train_f1_score = f1_score(y_train, y_train_pred_xgb, average='weighted')\n",
    "valid_f1_score = f1_score(y_valid, y_valid_pred_xgb, average='weighted')\n",
    "test_f1_score = f1_score(y_test, y_test_pred_xgb, average='weighted')\n",
    "\n",
    "# Precision scores\n",
    "train_precision = precision_score(y_train, y_train_pred_xgb, average='weighted')\n",
    "valid_precision = precision_score(y_valid, y_valid_pred_xgb, average='weighted')\n",
    "test_precision = precision_score(y_test, y_test_pred_xgb, average='weighted')\n",
    "\n",
    "# Recall scores\n",
    "train_recall = recall_score(y_train, y_train_pred_xgb, average='weighted')\n",
    "valid_recall = recall_score(y_valid, y_valid_pred_xgb, average='weighted')\n",
    "test_recall = recall_score(y_test, y_test_pred_xgb, average='weighted')\n",
    "\n",
    "# Output\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Valid Accuracy:  {valid_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy:  {test_accuracy:.4f}\\n\")\n",
    "\n",
    "print(f\"Train F1-score: {train_f1_score:.4f}\")\n",
    "print(f\"Valid F1-score:  {valid_f1_score:.4f}\")\n",
    "print(f\"Test F1-score:  {test_f1_score:.4f}\\n\")\n",
    "\n",
    "print(f\"Train Precision: {train_precision:.4f}\")\n",
    "print(f\"Valid Precision:  {valid_precision:.4f}\")\n",
    "print(f\"Test Precision:  {test_precision:.4f}\\n\")\n",
    "\n",
    "print(f\"Train Recall: {train_recall:.4f}\")\n",
    "print(f\"Valid Recall:  {valid_recall:.4f}\")\n",
    "print(f\"Test Recall:  {test_recall:.4f}\\n\")\n",
    "\n",
    "print(\"Classification Report (Test):\")\n",
    "print(classification_report(y_test, y_test_pred_xgb))\n",
    "\n",
    "print(\"Confusion Matrix (Test):\")\n",
    "print(confusion_matrix(y_test, y_test_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e7b413",
   "metadata": {},
   "source": [
    "## Part 2: Generate SHAP explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8c467ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: add 1 more sequential chain which interpret sentiment from textual data\n",
    "\n",
    "api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2eb1eb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import pandas as pd\n",
    "\n",
    "# Extract the model from the pipeline\n",
    "best_model = xgb_model\n",
    "\n",
    "# Create SHAP TreeExplainer using the extracted model\n",
    "explainer = shap.TreeExplainer(xgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72b50094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SHAP values from cache.\n"
     ]
    }
   ],
   "source": [
    "# Calculate SHAP values for the test data (excluding CustomerId)\n",
    "# This took more than 1 hour to run...\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "shap_output_path = '../../models/shap_output.pkl'\n",
    "\n",
    "if os.path.exists(shap_output_path):\n",
    "    # Load precomputed SHAP values\n",
    "    shap_values, expected_value = joblib.load(shap_output_path)\n",
    "    print(\"Loaded SHAP values from cache.\")\n",
    "else:\n",
    "    # Calculate SHAP values\n",
    "    shap_values = explainer.shap_values(X_test_vectorized_with_id.drop(columns=['id']))\n",
    "    expected_value = explainer.expected_value\n",
    "\n",
    "    # Save SHAP values and base value for reuse\n",
    "    joblib.dump((shap_values, expected_value), shap_output_path)\n",
    "    print(\"SHAP values calculated and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d18e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert SHAP values to DataFrame for easier manipulation\n",
    "## [:,:,1] means i want to get shap values for positive class...\n",
    "shap_df = pd.DataFrame(shap_values, columns=X_test_vectorized_with_id.drop(columns=['id']).columns)\n",
    "\n",
    "# Add 'CustomerId' column to shap_df for alignment\n",
    "shap_df['id'] = X_test_vectorized_with_id['id'].values\n",
    "\n",
    "# Initialize a dictionary to store the JSON structures\n",
    "json_structures = {}\n",
    "\n",
    "# Generate a JSON structure for each row in shap_df\n",
    "for index, row in shap_df.iterrows():\n",
    "    # Create a dictionary for the current row\n",
    "    row_dict = row.to_dict()\n",
    "\n",
    "    # Use CustomerId as the key for the JSON structure and remove it from the values\n",
    "    customer_id = row_dict.pop('id')\n",
    "    json_structures[customer_id] = row_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5118165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "import time\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import LLMChain, SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e526d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_and_get_top_features(features):\n",
    "    sorted_features = sorted(features.items(), key=lambda item: abs(item[1]), reverse=True)\n",
    "    top_features = sorted_features[:10]\n",
    "    return top_features \n",
    "\n",
    "# Create an empty DataFrame\n",
    "features_shap_values = pd.DataFrame()\n",
    "\n",
    "# Iterate over each ID key\n",
    "for id_key, features in json_structures.items():\n",
    "    sorted_features_df = sort_and_get_top_features(features)\n",
    "    keys = [key for key, _ in sorted_features_df]\n",
    "    values = [value for _, value in sorted_features_df]\n",
    "    features_shap_values = pd.concat([features_shap_values, pd.DataFrame({\"ID\": id_key,\n",
    "                                                                          \"top10_feature\": [keys],\n",
    "                                                                          \"top10_shap_values\":[values]})])\n",
    "                                     \n",
    "features_shap_values = features_shap_values.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be1fd0d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rf_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Predict labels and probabilities\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m labels \u001b[38;5;241m=\u001b[39m rf_model\u001b[38;5;241m.\u001b[39mpredict(X_test_vectorized)\n\u001b[0;32m      7\u001b[0m proba \u001b[38;5;241m=\u001b[39m rf_model\u001b[38;5;241m.\u001b[39mpredict_proba(X_test_vectorized)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Create predictions DataFrame\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rf_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Combine predict and predict_proba in a DataFrame\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Predict labels and probabilities\n",
    "labels = rf_model.predict(X_test_vectorized)\n",
    "proba = rf_model.predict_proba(X_test_vectorized)\n",
    "\n",
    "# Create predictions DataFrame\n",
    "predictions = pd.DataFrame(proba, columns=[f\"prediction_score_{cls}\" for cls in rf_model.classes_])\n",
    "predictions.insert(0, \"prediction_label\", labels)\n",
    "\n",
    "# Reset index if necessary (to ensure alignment during concat)\n",
    "X_test_vectorized_with_id = X_test_vectorized_with_id.reset_index(drop=True)\n",
    "predictions = predictions.reset_index(drop=True)\n",
    "\n",
    "# Combine features with predictions\n",
    "combined_df = pd.concat([X_test_vectorized_with_id, predictions], axis=1)\n",
    "\n",
    "# Convert to JSON (list of dicts)\n",
    "parsed_json = json.loads(combined_df.to_json(orient='records'))\n",
    "\n",
    "# Example output\n",
    "print(parsed_json[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f32c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_data(user_id, parsed_json, json_structures):\n",
    "    desired_data = next(item for item in parsed_json if item['id'] == int(user_id))\n",
    "\n",
    "    # get features from parsed_json, but exclude the text features\n",
    "    meta_data = {\n",
    "        'id': desired_data['id'],\n",
    "        'gender_M': desired_data['gender_M'],\n",
    "        'region_category_Town': desired_data['region_category_Town'],\n",
    "        'region_category_Village': desired_data['region_category_Village'],\n",
    "        'joined_through_referral_Yes': desired_data['joined_through_referral_Yes'],\n",
    "        'preferred_offer_types_Gift Vouchers/Coupons': desired_data['preferred_offer_types_Gift Vouchers/Coupons'],\n",
    "        'preferred_offer_types_Without Offers': desired_data['preferred_offer_types_Without Offers'],\n",
    "        'medium_of_operation_Desktop': desired_data['medium_of_operation_Desktop'],\n",
    "        'medium_of_operation_Smartphone': desired_data['medium_of_operation_Smartphone'],\n",
    "        'internet_option_Mobile_Data': desired_data['internet_option_Mobile_Data'],\n",
    "        'internet_option_Wi-Fi': desired_data['internet_option_Wi-Fi'],\n",
    "        'used_special_discount_Yes': desired_data['used_special_discount_Yes'],\n",
    "        'offer_application_preference_Yes': desired_data['offer_application_preference_Yes'],\n",
    "        'past_complaint_Yes': desired_data['past_complaint_Yes'],\n",
    "        'year': desired_data['year'],\n",
    "        'membership_category': desired_data['membership_category'],\n",
    "        'complaint_status': desired_data['complaint_status'],\n",
    "        'age': desired_data['age'],\n",
    "        'days_since_last_login': desired_data['days_since_last_login'],\n",
    "        'avg_time_spent': desired_data['avg_time_spent'],\n",
    "        'avg_transaction_value': desired_data['avg_transaction_value'],\n",
    "        'avg_frequency_login_days': desired_data['avg_frequency_login_days'],\n",
    "        'points_in_wallet': desired_data['points_in_wallet'],\n",
    "    }\n",
    "\n",
    "    print('metadata:')\n",
    "    print(meta_data)\n",
    "    print('\\n')\n",
    "\n",
    "    # get predicted_label from parsed_json\n",
    "    result = desired_data['prediction_label']\n",
    "    \n",
    "    shap_values = json_structures[int(user_id)]\n",
    "    \n",
    "    return meta_data, result, shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5c2eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_churn_explainability(meta_data: dict, \n",
    "                                  result: int, shap_values: dict, \n",
    "                                  ## input the customer feedback if you want to perform text analysis on customer feedback via LLM\n",
    "                                  customer_feedback: str):\n",
    "\n",
    "    # Create Model\n",
    "    llm = ChatOpenAI(model =\"gpt-4.1-mini\", temperature=0.4, openai_api_key = api_key)\n",
    "    \n",
    "    columns = ['gender_M', 'region_category_Town', 'region_category_Village', 'joined_through_referral_Yes', \n",
    "            'preferred_offer_types_Gift Vouchers/Coupons', 'preferred_offer_types_Without Offers',\n",
    "            'medium_of_operation_Desktop', 'medium_of_operation_Smartphone', 'internet_option_Mobile_Data', \n",
    "            'internet_option_Wi-Fi', 'used_special_discount_Yes', 'offer_application_preference_Yes',\n",
    "            'past_complaint_Yes', 'year', 'membership_category', 'complaint_status', 'age', 'days_since_last_login', \n",
    "            'avg_time_spent', 'avg_transaction_value', 'avg_frequency_login_days', 'points_in_wallet'\n",
    "    ]\n",
    "\n",
    "    # Get columns  for user \n",
    "    template0 = \"Explain the significance and potential usage of the following data columns in a dataset: {columns}. \\\n",
    "                  Provide a simple description for each column.\"\n",
    "    prompt0 = ChatPromptTemplate.from_template(template0)\n",
    "    chain_0 = LLMChain(llm=llm,\n",
    "                   prompt=prompt0,\n",
    "                   output_key=\"column_descriptions\")\n",
    "\n",
    "\n",
    "    # Template 1: Get meta data description for user \n",
    "    template1 = \"Explain {meta_data} based on the information provided by {column_descriptions}.\"\n",
    "    prompt1 = ChatPromptTemplate.from_template(template1)\n",
    "    chain_1 = LLMChain(llm=llm,\n",
    "                       prompt=prompt1,\n",
    "                       output_key=\"meta_data_description\")\n",
    "\n",
    "    # Template 2: Get explainability for the user\n",
    "    system_template = \"You are a churn problem explainability assistant. Your goal is to understand the  complexities\\\n",
    "                       of customer loss and uncover why customers are leaving a service or product in business models.\\\n",
    "                       The explanations should be simple and understandable for users to analyze.\"\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "    human_template = \"The churn result of a customer, which is {result} value has been obtained in a machine\\\n",
    "                     learning model using the {meta_data_description}. The {shap_values} represent the Shapley\\\n",
    "                     values for each feature, indicating why the model produced this particular result. Shapley\\\n",
    "                     values provide insights into the contribution of each feature to the final prediction. \\\n",
    "                     Explain why such a result was obtained in a way that the user can understand. Don't explain\\\n",
    "                     what the Shapley value is in a technical manner, using language that a non-technical\\\n",
    "                     person can understand. Maximum text length should be 200 words. Provide a concise explanation\\\n",
    "                     without using the phrase 'based on.\"\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "    prompt2 = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "    chain_2 = LLMChain(llm=llm,\n",
    "                       prompt=prompt2,\n",
    "                       output_key=\"explanation\")\n",
    "\n",
    "    ## Template 3: \n",
    "    template3 = \"Identify the main pain points, complaints, or issues mentioned in this customer feedback: {customer_feedback}. \\\n",
    "            Extract the specific problems the customer is experiencing and categorize them (e.g., product issues, \\\n",
    "            service problems, pricing concerns, usability issues).\"\n",
    "    prompt3 = ChatPromptTemplate.from_template(template3)\n",
    "    chain_3 = LLMChain(llm=llm,\n",
    "                           prompt=prompt3,\n",
    "                           output_key=\"sentiment\")\n",
    "    \n",
    "    # Template 4: \n",
    "    template4 = \"Provide strategies based on {explanation} to prevent user loss. Explain in language that a non-technical \\\n",
    "                 person can understand. Maximum text length should be 200 words. Provide a concise explanation without using the \\\n",
    "                 phrase 'based on.\"\n",
    "    prompt4 = ChatPromptTemplate.from_template(template4)\n",
    "    chain_4 = LLMChain(llm=llm,\n",
    "                       prompt=prompt4,\n",
    "                       output_key=\"recommendation\")\n",
    "    \n",
    "\n",
    "    # Define the  chain of operations with the new chain\n",
    "    seq_chain = SequentialChain(chains=[chain_0, chain_1, chain_2, chain_3, chain_4],\n",
    "                                input_variables=['columns', 'meta_data', 'result', 'shap_values', 'customer_feedback'],\n",
    "                                output_variables=['column_descriptions', 'meta_data_description', \n",
    "                                                  'explanation', 'sentiment', 'recommendation'],\n",
    "                                verbose=True)\n",
    "\n",
    "    input_data = {\n",
    "        'columns': columns,\n",
    "        'meta_data': meta_data,\n",
    "        'result': result,\n",
    "        'shap_values': shap_values,\n",
    "        'customer_feedback': customer_feedback\n",
    "    }\n",
    "\n",
    "    output = seq_chain.invoke(input_data)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832495fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_customer_id = 9809\n",
    "customer_feedback = X_test[X_test.id == sample_customer_id]['feedback'].iloc[0]\n",
    "\n",
    "meta_datas, results, shap_valuess = prepare_input_data(sample_customer_id, parsed_json, json_structures)\n",
    "churn_explainability_sample_1 = generate_churn_explainability(meta_datas, results, shap_valuess, customer_feedback)\n",
    "idx = features_shap_values.loc[features_shap_values.ID == sample_customer_id].index\n",
    "features_shap_values.loc[idx,'explanation'] = churn_explainability_sample_1['explanation']\n",
    "features_shap_values.loc[idx, 'sentiment'] = churn_explainability_sample_1['sentiment']\n",
    "features_shap_values.loc[idx, 'recommendation'] = churn_explainability_sample_1['recommendation']\n",
    "\n",
    "time.sleep(60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
