{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a69a63dc",
   "metadata": {},
   "source": [
    "# Evaluation on LLM-generated narratives for SHAP explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1355dc84",
   "metadata": {},
   "source": [
    "## Part 1: Data loading & Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70753b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the cleaned data\n",
    "import pandas as pd\n",
    "\n",
    "merged_data_final = pd.read_csv(\"../../data/processed/cleaned_data.csv.gz\", compression=\"gzip\")\n",
    "\n",
    "X = merged_data_final.drop(['churn_risk_score'], axis = 1)\n",
    "y = merged_data_final['churn_risk_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce49a7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_data_final.drop(['churn_risk_score'], axis = 1)\n",
    "y = merged_data_final['churn_risk_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f4870e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "                    X, y, train_size=0.6, \n",
    "                    stratify= y,\n",
    "                    random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "                    X_temp, y_temp, train_size=0.5,\n",
    "                    stratify=y_temp,\n",
    "                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92405feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export train and test dataset to `data` folder\n",
    "pd.concat([X_train, y_train], axis=1).to_csv(\"../../data/input/train.csv.gz\", index=False)\n",
    "pd.concat([X_valid, y_valid], axis=1).to_csv(\"../../data/input/valid.csv.gz\", index=False)\n",
    "pd.concat([X_test, y_test], axis=1).to_csv(\"../../data/input/test.csv.gz\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c23477b",
   "metadata": {},
   "source": [
    "### Text Representation with LLM embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a07a4dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "from utils.prepare_llm_embedding import generate_embeddings_from_series \n",
    "\n",
    "EMBEDDING_TRAIN = \"../../data/processed/llm_embedding_train.csv.gz\"\n",
    "if os.path.exists(EMBEDDING_TRAIN):\n",
    "    pass\n",
    "else:\n",
    "    processed_text_series = pd.Series(X_train[\"feedback\"].to_list(),\n",
    "                                    index=X_train['id'].to_list()) \n",
    "    llm_embedding_train = generate_embeddings_from_series(processed_text_series,\n",
    "                            additional_data={\"churn_risk_score\": y_train.to_list()},\n",
    "                            output_csv_path=\"../../data/processed/llm_embedding_train.csv.gz\",\n",
    "                            max_workers=20)\n",
    "    print(llm_embedding_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b37d21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "from utils.prepare_llm_embedding import generate_embeddings_from_series \n",
    "\n",
    "EMBEDDING_VALID = \"../../data/processed/llm_embedding_valid.csv.gz\"\n",
    "if os.path.exists(EMBEDDING_VALID):\n",
    "    pass\n",
    "else:\n",
    "    processed_text_series = pd.Series(X_valid[\"feedback\"].to_list(),\n",
    "                                    index=X_valid['id'].to_list()) \n",
    "    llm_embedding_valid = generate_embeddings_from_series(processed_text_series,\n",
    "                            additional_data={\"churn_risk_score\": y_valid.to_list()},\n",
    "                            output_csv_path=\"../../data/processed/llm_embedding_valid.csv.gz\",\n",
    "                            max_workers=20)\n",
    "    print(llm_embedding_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80b39d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "from utils.prepare_llm_embedding import generate_embeddings_from_series \n",
    "\n",
    "EMBEDDING_TEST = \"../../data/processed/llm_embedding_test.csv.gz\"\n",
    "if os.path.exists(EMBEDDING_TEST):\n",
    "    pass\n",
    "else:\n",
    "    processed_text_series = pd.Series(X_test[\"feedback\"].to_list(),\n",
    "                                    index=X_test['id'].to_list()) \n",
    "    llm_embedding_test= generate_embeddings_from_series(processed_text_series,\n",
    "                            additional_data={\"churn_risk_score\": y_test.to_list()},\n",
    "                            output_csv_path=\"../../data/processed/llm_embedding_test.csv.gz\",\n",
    "                            max_workers=20)\n",
    "    print(llm_embedding_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414654d8",
   "metadata": {},
   "source": [
    "### Combine LLM embeddings with structured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e421379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_vectorized = pd.read_csv(\"../../data/processed/llm_embedding_train.csv.gz\", compression=\"gzip\")\n",
    "valid_text_vectorized = pd.read_csv(\"../../data/processed/llm_embedding_valid.csv.gz\", compression=\"gzip\")\n",
    "test_text_vectorized = pd.read_csv(\"../../data/processed/llm_embedding_test.csv.gz\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49807123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Create train df which combines all structured data and textual data\n",
    "train_df = pd.merge(X_train, train_text_vectorized[['id', 'embedding_json', 'churn_risk_score']], on='id', how='outer')\n",
    "train_df['embedding_json'] = train_df['embedding_json'].apply(ast.literal_eval)\n",
    "\n",
    "# create dataframe with columns with 'unstructured data'\n",
    "X_train_unstructured_llm_embedding = pd.DataFrame(train_df['embedding_json'].tolist())\n",
    "X_train_unstructured_llm_embedding.columns = [f\"text_feature_{i+1}\" for i in range(X_train_unstructured_llm_embedding.shape[1])]\n",
    "\n",
    "# create dataframe with columns with 'structured data'\n",
    "X_train_structured_with_id = train_df.drop(columns=['feedback', 'embedding_json', 'churn_risk_score'])\n",
    "X_train_structured = X_train_structured_with_id.drop(columns=[\"id\"])\n",
    "\n",
    "# Concatenate with both dataframe with structured and unstructured data\n",
    "X_train_vectorized = pd.concat([X_train_structured_with_id, X_train_unstructured_llm_embedding], axis=1).drop(columns=['id'])\n",
    "X_train_vectorized_with_id = pd.concat([X_train_structured_with_id, X_train_unstructured_llm_embedding], axis=1)\n",
    "\n",
    "# Create target variable\n",
    "y_train = train_df['churn_risk_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8f0bb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation df which combines all structured data and textual data\n",
    "valid_df = pd.merge(X_valid, valid_text_vectorized[['id', 'embedding_json', 'churn_risk_score']], on='id', how='outer')\n",
    "valid_df['embedding_json'] = valid_df['embedding_json'].apply(ast.literal_eval)\n",
    "\n",
    "# create dataframe with columns with 'unstructured data'\n",
    "X_valid_unstructured_llm_embedding = pd.DataFrame(valid_df['embedding_json'].tolist())\n",
    "X_valid_unstructured_llm_embedding.columns = [f\"text_feature_{i+1}\" for i in range(X_valid_unstructured_llm_embedding.shape[1])]\n",
    "\n",
    "# create dataframe with columns with 'structured data'\n",
    "X_valid_structured_with_id = valid_df.drop(columns=['feedback', 'embedding_json', 'churn_risk_score'])\n",
    "X_valid_structured = X_valid_structured_with_id.drop(columns=[\"id\"])\n",
    "\n",
    "# Concatenate with both dataframe with structured and unstructured data\n",
    "X_valid_vectorized  = pd.concat([X_valid_structured_with_id, X_valid_unstructured_llm_embedding], axis=1).drop(columns=['id'])\n",
    "X_valid_vectorized_with_id  = pd.concat([X_valid_structured_with_id, X_valid_unstructured_llm_embedding], axis=1)\n",
    "\n",
    "# Create target variable\n",
    "y_valid = valid_df['churn_risk_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dab5558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test df which combines all structured data and textual data\n",
    "test_df = pd.merge(X_test, test_text_vectorized[['id', 'embedding_json', 'churn_risk_score']], on='id', how='outer')\n",
    "test_df['embedding_json'] = test_df['embedding_json'].apply(ast.literal_eval)\n",
    "\n",
    "# create dataframe with columns with 'unstructured data'\n",
    "X_test_unstructured_llm_embedding = pd.DataFrame(test_df['embedding_json'].tolist())\n",
    "X_test_unstructured_llm_embedding.columns = [f\"text_feature_{i+1}\" for i in range(X_test_unstructured_llm_embedding.shape[1])]\n",
    "\n",
    "# create dataframe with columns with 'structured data'\n",
    "X_test_structured_with_id = test_df.drop(columns=['feedback', 'embedding_json', 'churn_risk_score'])\n",
    "X_test_structured = X_test_structured_with_id.drop(columns=[\"id\"])\n",
    "\n",
    "## Concatenate with original dataframe (drop embedding_json)\n",
    "X_test_vectorized = pd.concat([X_test_structured_with_id, X_test_unstructured_llm_embedding], axis=1).drop(columns=['id'])\n",
    "X_test_vectorized_with_id = pd.concat([X_test_structured_with_id, X_test_unstructured_llm_embedding], axis=1)\n",
    "\n",
    "# Create target variable\n",
    "y_test = test_df['churn_risk_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731b5ea4",
   "metadata": {},
   "source": [
    "### Pick the best model - \"XGBoost with Structured Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "821c6e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 1.0000\n",
      "Valid Accuracy:  0.9273\n",
      "Test Accuracy:  0.9273\n",
      "\n",
      "Train F1-score: 1.0000\n",
      "Valid F1-score:  0.9271\n",
      "Test F1-score:  0.9272\n",
      "\n",
      "Train Precision: 1.0000\n",
      "Valid Precision:  0.9279\n",
      "Test Precision:  0.9275\n",
      "\n",
      "Train Recall: 1.0000\n",
      "Valid Recall:  0.9273\n",
      "Test Recall:  0.9273\n",
      "\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.90      0.92      3396\n",
      "           1       0.92      0.95      0.93      4003\n",
      "\n",
      "    accuracy                           0.93      7399\n",
      "   macro avg       0.93      0.93      0.93      7399\n",
      "weighted avg       0.93      0.93      0.93      7399\n",
      "\n",
      "Confusion Matrix (Test):\n",
      "[[3070  326]\n",
      " [ 212 3791]]\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import (f1_score, accuracy_score, precision_score, \n",
    "                            recall_score, classification_report, confusion_matrix)\n",
    "\n",
    "# Train the model\n",
    "xgb_model = xgb.XGBClassifier(max_depth=10,\n",
    "                            random_state=42,\n",
    "                            # Introduce randomness to make training faster and reduce overfitting\n",
    "                            subsample=0.8, ## Uses 80% of the data for each tree.\n",
    "                            colsample_bytree=0.8, ## Uses 80% of the features for each tree.\n",
    "                            # the parameters below make the model trained faster by enabling parallelism\n",
    "                            n_jobs = -1)\n",
    "xgb_model.fit(X_train_structured, y_train)\n",
    "\n",
    "# Predictions on training and test sets\n",
    "y_train_pred_xgb = xgb_model.predict(X_train_structured)\n",
    "y_valid_pred_xgb = xgb_model.predict(X_valid_structured)\n",
    "y_test_pred_xgb = xgb_model.predict(X_test_structured)\n",
    "\n",
    "# Accuracy scores\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred_xgb)\n",
    "valid_accuracy = accuracy_score(y_valid, y_valid_pred_xgb)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred_xgb)\n",
    "\n",
    "# F1 scores\n",
    "train_f1_score = f1_score(y_train, y_train_pred_xgb, average='weighted')\n",
    "valid_f1_score = f1_score(y_valid, y_valid_pred_xgb, average='weighted')\n",
    "test_f1_score = f1_score(y_test, y_test_pred_xgb, average='weighted')\n",
    "\n",
    "# Precision scores\n",
    "train_precision = precision_score(y_train, y_train_pred_xgb, average='weighted')\n",
    "valid_precision = precision_score(y_valid, y_valid_pred_xgb, average='weighted')\n",
    "test_precision = precision_score(y_test, y_test_pred_xgb, average='weighted')\n",
    "\n",
    "# Recall scores\n",
    "train_recall = recall_score(y_train, y_train_pred_xgb, average='weighted')\n",
    "valid_recall = recall_score(y_valid, y_valid_pred_xgb, average='weighted')\n",
    "test_recall = recall_score(y_test, y_test_pred_xgb, average='weighted')\n",
    "\n",
    "# Output\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Valid Accuracy:  {valid_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy:  {test_accuracy:.4f}\\n\")\n",
    "\n",
    "print(f\"Train F1-score: {train_f1_score:.4f}\")\n",
    "print(f\"Valid F1-score:  {valid_f1_score:.4f}\")\n",
    "print(f\"Test F1-score:  {test_f1_score:.4f}\\n\")\n",
    "\n",
    "print(f\"Train Precision: {train_precision:.4f}\")\n",
    "print(f\"Valid Precision:  {valid_precision:.4f}\")\n",
    "print(f\"Test Precision:  {test_precision:.4f}\\n\")\n",
    "\n",
    "print(f\"Train Recall: {train_recall:.4f}\")\n",
    "print(f\"Valid Recall:  {valid_recall:.4f}\")\n",
    "print(f\"Test Recall:  {test_recall:.4f}\\n\")\n",
    "\n",
    "print(\"Classification Report (Test):\")\n",
    "print(classification_report(y_test, y_test_pred_xgb))\n",
    "\n",
    "print(\"Confusion Matrix (Test):\")\n",
    "print(confusion_matrix(y_test, y_test_pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e7b413",
   "metadata": {},
   "source": [
    "## Part 2: Generate SHAP explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109efaf6",
   "metadata": {},
   "source": [
    "### 2.1 Prepare SHAP values to be feed into LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b1c6bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP values calculated\n"
     ]
    }
   ],
   "source": [
    "# Calculate SHAP values for the test data (excluding CustomerId)\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "import shap\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "n_samples = 30\n",
    "\n",
    "# Extract the model from the pipeline\n",
    "best_model = xgb_model\n",
    "\n",
    "# Create SHAP TreeExplainer using the extracted model\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "\n",
    "# Perform stratified sampling on the test data to select 'n_samples' instances\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=n_samples, random_state=42)\n",
    "for _, test_index in sss.split(X_test_structured_with_id, y_test):\n",
    "    X_test_sampled_with_id = X_test_structured_with_id.iloc[test_index]\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values = explainer.shap_values(X_test_sampled_with_id.drop(columns=['id']))\n",
    "expected_value = explainer.expected_value\n",
    "\n",
    "print(\"SHAP values calculated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d18e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert SHAP values to DataFrame for easier manipulation\n",
    "shap_df = pd.DataFrame(shap_values, columns=X_test_structured_with_id.drop(columns=['id']).columns)\n",
    "\n",
    "# Add 'Id' column to shap_df for alignment\n",
    "shap_df['id'] = X_test_sampled_with_id['id'].values\n",
    "\n",
    "# Initialize a dictionary to store the JSON structures\n",
    "json_structures = {}\n",
    "\n",
    "# Generate a JSON structure for each row in shap_df\n",
    "for index, row in shap_df.iterrows():\n",
    "    # Create a dictionary for the current row\n",
    "    row_dict = row.to_dict()\n",
    "\n",
    "    # Use id as the key for the JSON structure and remove it from the values\n",
    "    customer_id = row_dict.pop('id')\n",
    "    json_structures[customer_id] = row_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62e561cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_and_get_top_features(features):\n",
    "    sorted_features = sorted(features.items(), key=lambda item: abs(item[1]), reverse=True)\n",
    "    top_features = sorted_features[:10]\n",
    "    return top_features \n",
    "\n",
    "# Create an empty DataFrame\n",
    "features_shap_values = pd.DataFrame()\n",
    "\n",
    "# Iterate over each ID key\n",
    "for id_key, features in json_structures.items():\n",
    "    sorted_features_df = sort_and_get_top_features(features)\n",
    "    keys = [key for key, _ in sorted_features_df]\n",
    "    values = [value for _, value in sorted_features_df]\n",
    "    features_shap_values = pd.concat([features_shap_values, pd.DataFrame({\"ID\": id_key,\n",
    "                                                                          \"top10_feature\": [keys],\n",
    "                                                                          \"top10_shap_values\":[values]})])\n",
    "                                     \n",
    "features_shap_values = features_shap_values.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2df494b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'gender_M': 0, 'region_category_Town': 0, 'region_category_Village': 0, 'joined_through_referral_Yes': 0, 'preferred_offer_types_Gift Vouchers/Coupons': 1, 'preferred_offer_types_Without Offers': 0, 'medium_of_operation_Desktop': 1, 'medium_of_operation_Smartphone': 0, 'internet_option_Mobile_Data': 1, 'internet_option_Wi-Fi': 0, 'used_special_discount_Yes': 1, 'offer_application_preference_Yes': 0, 'past_complaint_Yes': 1, 'id': 2, 'years_since_joining': 4, 'membership_category': 5.0, 'complaint_status': 3.0, 'age': -0.3225616825, 'days_since_last_login': 0.5927148131, 'avg_time_spent': -0.8047866074, 'avg_transaction_value': -0.8451602355, 'avg_frequency_login_days': -0.8803947756, 'points_in_wallet': 0.0337832844, 'text_feature_1': 0.02197793, 'text_feature_2': -0.046620663, 'text_feature_3': -0.03335001, 'text_feature_4': -0.03717942, 'text_feature_5': 0.012610546, 'text_feature_6': -0.005627555, 'text_feature_7': 0.010386296, 'text_feature_8': -0.022074727, 'text_feature_9': -0.03445136, 'text_feature_10': -0.04589714, 'text_feature_11': -0.025709337, 'text_feature_12': 0.045858815, 'text_feature_13': 0.10565658, 'text_feature_14': -0.024765218, 'text_feature_15': -0.048870184, 'text_feature_16': -0.040169653, 'text_feature_17': 0.044822637, 'text_feature_18': 0.026948756, 'text_feature_19': -0.08310361, 'text_feature_20': -0.027554186, 'text_feature_21': -0.02292592, 'text_feature_22': -0.021360934, 'text_feature_23': 0.020064814, 'text_feature_24': -0.039817892, 'text_feature_25': 0.017534386, 'text_feature_26': 0.014907786, 'text_feature_27': 0.05028059, 'text_feature_28': 0.044555314, 'text_feature_29': -0.027165923, 'text_feature_30': -0.06444482, 'text_feature_31': 0.026132206, 'text_feature_32': -0.010562403, 'text_feature_33': -0.009229659, 'text_feature_34': 0.0052298773, 'text_feature_35': 0.025877774, 'text_feature_36': 0.013410763, 'text_feature_37': -0.058862373, 'text_feature_38': 0.03177279, 'text_feature_39': 0.014743508, 'text_feature_40': -0.051016197, 'text_feature_41': -0.03210137, 'text_feature_42': 0.05533713, 'text_feature_43': -0.036389645, 'text_feature_44': -0.0114569105, 'text_feature_45': 0.012884089, 'text_feature_46': -0.0001812458, 'text_feature_47': -0.004476723, 'text_feature_48': 0.032857552, 'text_feature_49': 0.022859704, 'text_feature_50': 0.036233682, 'text_feature_51': 0.0026080913, 'text_feature_52': 0.0139939645, 'text_feature_53': -0.038912702, 'text_feature_54': -0.017218987, 'text_feature_55': -0.012914944, 'text_feature_56': -0.02690335, 'text_feature_57': -0.048635025, 'text_feature_58': -0.042526282, 'text_feature_59': 0.10489853, 'text_feature_60': -0.043493975, 'text_feature_61': 0.0075107203, 'text_feature_62': -0.0014452153, 'text_feature_63': 0.025005, 'text_feature_64': 0.0014356602, 'text_feature_65': -0.0016074048, 'text_feature_66': -0.07286875, 'text_feature_67': 0.028137667, 'text_feature_68': -0.06235186, 'text_feature_69': -0.050614655, 'text_feature_70': -0.014447576, 'text_feature_71': -0.0067339186, 'text_feature_72': 0.06007355, 'text_feature_73': -0.013424382, 'text_feature_74': -0.07285552, 'text_feature_75': 0.0004485473, 'text_feature_76': -0.058971986, 'text_feature_77': 0.0051061204, 'text_feature_78': -0.030058004, 'text_feature_79': -0.010935411, 'text_feature_80': 0.02403329, 'text_feature_81': 0.025243538, 'text_feature_82': 0.02268903, 'text_feature_83': 0.049997248, 'text_feature_84': -0.010897141, 'text_feature_85': 0.04869854, 'text_feature_86': -0.03405632, 'text_feature_87': -0.05304376, 'text_feature_88': -0.006782621, 'text_feature_89': -0.063170336, 'text_feature_90': -0.0049304506, 'text_feature_91': 0.026294447, 'text_feature_92': 0.06333668, 'text_feature_93': -0.042734977, 'text_feature_94': 0.021656487, 'text_feature_95': 0.05933057, 'text_feature_96': 0.020244451, 'text_feature_97': -0.044130307, 'text_feature_98': -0.028853895, 'text_feature_99': 0.04377088, 'text_feature_100': 0.0073371576, 'text_feature_101': 0.024248198, 'text_feature_102': 0.014779695, 'text_feature_103': 0.083973154, 'text_feature_104': -0.044876147, 'text_feature_105': -0.029317053, 'text_feature_106': -0.04801157, 'text_feature_107': -0.046006713, 'text_feature_108': -0.031744067, 'text_feature_109': -0.07404389, 'text_feature_110': 0.049574383, 'text_feature_111': -0.047174606, 'text_feature_112': -0.023222333, 'text_feature_113': 0.011664649, 'text_feature_114': 0.017639847, 'text_feature_115': -0.0003312813, 'text_feature_116': 0.0049609966, 'text_feature_117': 0.02931884, 'text_feature_118': 0.009377561, 'text_feature_119': 5.23632e-05, 'text_feature_120': 0.026499275, 'text_feature_121': -0.0040342696, 'text_feature_122': 0.074490726, 'text_feature_123': -0.015368038, 'text_feature_124': 0.06574092, 'text_feature_125': 0.06191719, 'text_feature_126': 0.013164579, 'text_feature_127': 0.0057015987, 'text_feature_128': -0.0123098185, 'text_feature_129': -0.0010975971, 'text_feature_130': -0.0843898, 'text_feature_131': -0.042064242, 'text_feature_132': -0.060752984, 'text_feature_133': 0.0077339928, 'text_feature_134': 0.044029772, 'text_feature_135': 0.008326635, 'text_feature_136': -0.0131395385, 'text_feature_137': 0.034426987, 'text_feature_138': -0.09120517, 'text_feature_139': 0.011503513, 'text_feature_140': -0.01870325, 'text_feature_141': -0.001002652, 'text_feature_142': -0.058018677, 'text_feature_143': -0.0077787987, 'text_feature_144': 0.057956144, 'text_feature_145': 0.0050265533, 'text_feature_146': 0.0019296346, 'text_feature_147': -0.017978704, 'text_feature_148': 0.069701254, 'text_feature_149': 0.023076026, 'text_feature_150': 0.09426339, 'text_feature_151': 0.014365912, 'text_feature_152': -0.027132768, 'text_feature_153': -0.0072358972, 'text_feature_154': -0.015012904, 'text_feature_155': 0.023208201, 'text_feature_156': -0.012129965, 'text_feature_157': 0.008813273, 'text_feature_158': -0.001734514, 'text_feature_159': 0.041340653, 'text_feature_160': 0.01925858, 'text_feature_161': 0.05064405, 'text_feature_162': 0.0026022634, 'text_feature_163': 0.023432102, 'text_feature_164': -0.0029329634, 'text_feature_165': -0.036741998, 'text_feature_166': -0.038367253, 'text_feature_167': -0.019877922, 'text_feature_168': -0.03712997, 'text_feature_169': -0.025725124, 'text_feature_170': -0.006983937, 'text_feature_171': -0.012338232, 'text_feature_172': -0.014358473, 'text_feature_173': 0.010488939, 'text_feature_174': -0.09454249, 'text_feature_175': -0.041193124, 'text_feature_176': 0.023326328, 'text_feature_177': 0.015535287, 'text_feature_178': -0.0130453175, 'text_feature_179': -0.04262824, 'text_feature_180': 0.059087284, 'text_feature_181': 0.07148073, 'text_feature_182': 0.055384543, 'text_feature_183': -0.0077512213, 'text_feature_184': -0.0039415816, 'text_feature_185': 5.0622e-05, 'text_feature_186': -0.004815686, 'text_feature_187': -0.0771075, 'text_feature_188': 0.0054298607, 'text_feature_189': 0.05682347, 'text_feature_190': -0.0035050146, 'text_feature_191': 0.05843864, 'text_feature_192': -0.050367538, 'text_feature_193': -0.018275812, 'text_feature_194': 0.055597734, 'text_feature_195': -0.061085656, 'text_feature_196': 0.0285289, 'text_feature_197': 0.06409461, 'text_feature_198': -0.040056683, 'text_feature_199': 0.01759263, 'text_feature_200': -0.022328937, 'text_feature_201': 0.026584458, 'text_feature_202': 0.027764061, 'text_feature_203': 0.0054423274, 'text_feature_204': -0.03722546, 'text_feature_205': 0.028519271, 'text_feature_206': 0.0015680904, 'text_feature_207': 0.005068008, 'text_feature_208': 0.035562795, 'text_feature_209': 0.04478437, 'text_feature_210': -0.04951605, 'text_feature_211': -0.007693317, 'text_feature_212': 0.031448178, 'text_feature_213': 0.018551424, 'text_feature_214': -0.009732465, 'text_feature_215': 0.02603647, 'text_feature_216': -0.06385336, 'text_feature_217': 0.07544759, 'text_feature_218': -0.012728267, 'text_feature_219': 0.006236384, 'text_feature_220': -0.014981585, 'text_feature_221': -0.0030337418, 'text_feature_222': -0.005028808, 'text_feature_223': 0.019637266, 'text_feature_224': 0.029590894, 'text_feature_225': -0.008667372, 'text_feature_226': 0.022362214, 'text_feature_227': -0.09261708, 'text_feature_228': 0.007981307, 'text_feature_229': -0.016285129, 'text_feature_230': -0.03770608, 'text_feature_231': 0.01944795, 'text_feature_232': 0.03423988, 'text_feature_233': -0.055476706, 'text_feature_234': -0.02235609, 'text_feature_235': 0.0004368033, 'text_feature_236': 0.06930897, 'text_feature_237': -0.029188603, 'text_feature_238': -0.018825162, 'text_feature_239': 0.03599397, 'text_feature_240': 0.013313299, 'text_feature_241': -0.050752025, 'text_feature_242': 0.007552002, 'text_feature_243': 0.060887687, 'text_feature_244': -0.030758586, 'text_feature_245': -0.012216555, 'text_feature_246': -0.04379719, 'text_feature_247': 0.064239256, 'text_feature_248': 0.0433056, 'text_feature_249': 0.009524822, 'text_feature_250': -0.024404528, 'text_feature_251': 0.00854717, 'text_feature_252': 0.009373789, 'text_feature_253': 0.07895002, 'text_feature_254': 0.04988922, 'text_feature_255': -0.0071885698, 'text_feature_256': -0.009683647, 'text_feature_257': 0.05924308, 'text_feature_258': -0.03628732, 'text_feature_259': -0.015673121, 'text_feature_260': -0.034164995, 'text_feature_261': 0.0587762, 'text_feature_262': -0.040681325, 'text_feature_263': -0.03219103, 'text_feature_264': -0.013988608, 'text_feature_265': -0.050486278, 'text_feature_266': -0.034225494, 'text_feature_267': -0.03577934, 'text_feature_268': 0.032907445, 'text_feature_269': 0.011885589, 'text_feature_270': 0.033624753, 'text_feature_271': 0.00822606, 'text_feature_272': 0.016975746, 'text_feature_273': -0.031573825, 'text_feature_274': -0.01802913, 'text_feature_275': -0.008874963, 'text_feature_276': -0.006299911, 'text_feature_277': -0.030425586, 'text_feature_278': 0.0019722455, 'text_feature_279': -0.022740027, 'text_feature_280': 0.006716724, 'text_feature_281': 0.0060686823, 'text_feature_282': -0.017353315, 'text_feature_283': -0.051770065, 'text_feature_284': 0.013723515, 'text_feature_285': 0.025264343, 'text_feature_286': -0.04409338, 'text_feature_287': -0.05812656, 'text_feature_288': 0.018715534, 'text_feature_289': -0.008511918, 'text_feature_290': -0.008902193, 'text_feature_291': -0.017137727, 'text_feature_292': 0.009626028, 'text_feature_293': 0.048630733, 'text_feature_294': -0.039818835, 'text_feature_295': 0.037097145, 'text_feature_296': -0.03794866, 'text_feature_297': 0.018269068, 'text_feature_298': 0.051300973, 'text_feature_299': -0.0066523235, 'text_feature_300': 0.017318036, 'text_feature_301': 0.056282856, 'text_feature_302': -0.033499174, 'text_feature_303': -0.01926166, 'text_feature_304': 0.06586827, 'text_feature_305': -0.0048129703, 'text_feature_306': 0.048821658, 'text_feature_307': 0.09150447, 'text_feature_308': 0.032318346, 'text_feature_309': -0.020084336, 'text_feature_310': 0.057215184, 'text_feature_311': -0.019779406, 'text_feature_312': 0.0018587019, 'text_feature_313': -0.053935476, 'text_feature_314': -0.016340628, 'text_feature_315': -0.025241425, 'text_feature_316': 0.032039665, 'text_feature_317': -0.091844216, 'text_feature_318': 0.010977837, 'text_feature_319': -0.018781688, 'text_feature_320': -0.02092556, 'text_feature_321': 0.031458184, 'text_feature_322': -0.031445753, 'text_feature_323': -0.01581124, 'text_feature_324': 0.008043778, 'text_feature_325': -0.0076414896, 'text_feature_326': 0.022126531, 'text_feature_327': 0.049166217, 'text_feature_328': 0.0028457758, 'text_feature_329': -0.0037210349, 'text_feature_330': -0.007955001, 'text_feature_331': 0.024316277, 'text_feature_332': -0.019508354, 'text_feature_333': 0.051392775, 'text_feature_334': -0.036275364, 'text_feature_335': -0.05646041, 'text_feature_336': 0.033045247, 'text_feature_337': -0.047003917, 'text_feature_338': -0.009892498, 'text_feature_339': 0.058460254, 'text_feature_340': 0.016784763, 'text_feature_341': 0.035691038, 'text_feature_342': 0.007804025, 'text_feature_343': 0.046722125, 'text_feature_344': 0.009381178, 'text_feature_345': -0.022420317, 'text_feature_346': -0.053886153, 'text_feature_347': -0.012458609, 'text_feature_348': -0.009862397, 'text_feature_349': -0.018646613, 'text_feature_350': -0.028808406, 'text_feature_351': 0.06145114, 'text_feature_352': 0.0047968747, 'text_feature_353': -0.0017785768, 'text_feature_354': -0.052980397, 'text_feature_355': -0.008245572, 'text_feature_356': -0.008987959, 'text_feature_357': -0.004586542, 'text_feature_358': 0.04323389, 'text_feature_359': 0.03321394, 'text_feature_360': -0.0024017254, 'text_feature_361': 0.0028739518, 'text_feature_362': 0.047107775, 'text_feature_363': -0.014359631, 'text_feature_364': 0.13784379, 'text_feature_365': 0.0035739448, 'text_feature_366': 0.026206406, 'text_feature_367': 0.016478151, 'text_feature_368': 0.034052055, 'text_feature_369': 0.010498413, 'text_feature_370': -0.030267917, 'text_feature_371': -0.034919366, 'text_feature_372': 0.021417372, 'text_feature_373': 0.017349005, 'text_feature_374': 0.013501937, 'text_feature_375': 0.009805638, 'text_feature_376': -0.03978583, 'text_feature_377': -0.038404427, 'text_feature_378': 0.054891124, 'text_feature_379': 0.017393835, 'text_feature_380': -0.024029672, 'text_feature_381': -0.01927368, 'text_feature_382': -0.052250143, 'text_feature_383': 0.023914441, 'text_feature_384': 0.015976656, 'text_feature_385': 0.0022410885, 'text_feature_386': -0.020394243, 'text_feature_387': -0.11707732, 'text_feature_388': 0.042048212, 'text_feature_389': 0.041308597, 'text_feature_390': 0.007711862, 'text_feature_391': -0.014912195, 'text_feature_392': 0.05069366, 'text_feature_393': 0.006718512, 'text_feature_394': 0.028062565, 'text_feature_395': -0.030921245, 'text_feature_396': 0.037376285, 'text_feature_397': -0.043171424, 'text_feature_398': -0.017107002, 'text_feature_399': -0.0012897791, 'text_feature_400': 0.0022488434, 'text_feature_401': 0.041216858, 'text_feature_402': -0.03235808, 'text_feature_403': -0.038235486, 'text_feature_404': -0.0054005603, 'text_feature_405': 0.010337509, 'text_feature_406': 0.0076066586, 'text_feature_407': 0.05634424, 'text_feature_408': -0.0005772113, 'text_feature_409': 0.020254975, 'text_feature_410': -0.002095466, 'text_feature_411': -0.0055000503, 'text_feature_412': -0.014110249, 'text_feature_413': -0.056824956, 'text_feature_414': 0.024082405, 'text_feature_415': 0.0049008317, 'text_feature_416': 0.007898681, 'text_feature_417': -0.02567133, 'text_feature_418': 0.033593163, 'text_feature_419': -0.0026473578, 'text_feature_420': 0.01776485, 'text_feature_421': 0.022768179, 'text_feature_422': -0.027057074, 'text_feature_423': 0.061654564, 'text_feature_424': 0.023767188, 'text_feature_425': -0.01837718, 'text_feature_426': 0.03168071, 'text_feature_427': -0.05621053, 'text_feature_428': 0.015046035, 'text_feature_429': -0.016056357, 'text_feature_430': 0.02753845, 'text_feature_431': 0.04239153, 'text_feature_432': 0.08171149, 'text_feature_433': -0.030260632, 'text_feature_434': 0.010077429, 'text_feature_435': 0.032593142, 'text_feature_436': 0.002373992, 'text_feature_437': 0.02639761, 'text_feature_438': -0.025181832, 'text_feature_439': 0.05149665, 'text_feature_440': -0.01459654, 'text_feature_441': -0.0076843523, 'text_feature_442': 0.068386875, 'text_feature_443': 0.046136223, 'text_feature_444': -0.017712861, 'text_feature_445': -0.014742762, 'text_feature_446': 0.08465958, 'text_feature_447': 0.0584319, 'text_feature_448': -0.013913237, 'text_feature_449': -0.032236516, 'text_feature_450': -0.020542221, 'text_feature_451': 0.053358942, 'text_feature_452': 0.0027850398, 'text_feature_453': 0.025082452, 'text_feature_454': -0.037360057, 'text_feature_455': -0.04038807, 'text_feature_456': 0.063194394, 'text_feature_457': -0.03778539, 'text_feature_458': 0.027410436, 'text_feature_459': -0.062077805, 'text_feature_460': -0.014347364, 'text_feature_461': -0.06253476, 'text_feature_462': -0.011769658, 'text_feature_463': 0.037707504, 'text_feature_464': 0.026039673, 'text_feature_465': 0.01643177, 'text_feature_466': -0.0008690436, 'text_feature_467': -0.031789877, 'text_feature_468': 0.03568223, 'text_feature_469': -0.0021220006, 'text_feature_470': 0.08373024, 'text_feature_471': 0.019313866, 'text_feature_472': -0.0071959384, 'text_feature_473': 0.041983098, 'text_feature_474': -0.010040275, 'text_feature_475': 0.0055933977, 'text_feature_476': -0.022091944, 'text_feature_477': 0.0038420958, 'text_feature_478': -0.029184775, 'text_feature_479': -0.040028196, 'text_feature_480': -0.028997105, 'text_feature_481': -0.014046456, 'text_feature_482': -0.012530194, 'text_feature_483': 0.038285583, 'text_feature_484': 0.022798635, 'text_feature_485': 0.045666523, 'text_feature_486': 0.0113252215, 'text_feature_487': 0.019268673, 'text_feature_488': 0.0004766975, 'text_feature_489': -0.017478365, 'text_feature_490': 0.016864667, 'text_feature_491': 0.0079006115, 'text_feature_492': -0.005981455, 'text_feature_493': -0.019458024, 'text_feature_494': 0.01207623, 'text_feature_495': -0.06429754, 'text_feature_496': 0.008491677, 'text_feature_497': 0.08902788, 'text_feature_498': 0.036901046, 'text_feature_499': -0.013888895, 'text_feature_500': 0.053832337, 'text_feature_501': 0.04026082, 'text_feature_502': 0.05887912, 'text_feature_503': -0.008082446, 'text_feature_504': -0.06774728, 'text_feature_505': -0.029037675, 'text_feature_506': 0.018732116, 'text_feature_507': 0.015912313, 'text_feature_508': 0.019890059, 'text_feature_509': 0.044138204, 'text_feature_510': 0.031320766, 'text_feature_511': 0.010910761, 'text_feature_512': 0.031225936, 'text_feature_513': 0.0009403423, 'text_feature_514': 0.0041881544, 'text_feature_515': -0.024992902, 'text_feature_516': -0.013336194, 'text_feature_517': -0.0073324232, 'text_feature_518': 0.0030729221, 'text_feature_519': 0.0180229, 'text_feature_520': 0.057477996, 'text_feature_521': -0.03534637, 'text_feature_522': -0.03329728, 'text_feature_523': 0.008015954, 'text_feature_524': -0.084763676, 'text_feature_525': -0.028431427, 'text_feature_526': -0.031081524, 'text_feature_527': 0.05783543, 'text_feature_528': -0.01642735, 'text_feature_529': -0.029417755, 'text_feature_530': 0.042055223, 'text_feature_531': 0.024935842, 'text_feature_532': 0.04288877, 'text_feature_533': 0.029228976, 'text_feature_534': 0.01879097, 'text_feature_535': -0.030897226, 'text_feature_536': 0.028349638, 'text_feature_537': 0.07679094, 'text_feature_538': -0.032873273, 'text_feature_539': 0.042337555, 'text_feature_540': 0.055008605, 'text_feature_541': -0.010884419, 'text_feature_542': -0.035122823, 'text_feature_543': 0.040955175, 'text_feature_544': 0.022462117, 'text_feature_545': 0.013197952, 'text_feature_546': -0.00540105, 'text_feature_547': 0.039802246, 'text_feature_548': 0.023833903, 'text_feature_549': -0.0075339945, 'text_feature_550': 0.06903981, 'text_feature_551': -0.069488205, 'text_feature_552': -0.0038633242, 'text_feature_553': -0.015592579, 'text_feature_554': 0.038426865, 'text_feature_555': 0.015248055, 'text_feature_556': -0.008563961, 'text_feature_557': 0.0020925973, 'text_feature_558': 0.005847778, 'text_feature_559': 0.0130639905, 'text_feature_560': -0.032832988, 'text_feature_561': 0.060424216, 'text_feature_562': -0.044639066, 'text_feature_563': 0.05630719, 'text_feature_564': -0.0001108583, 'text_feature_565': 0.02687598, 'text_feature_566': -0.026032897, 'text_feature_567': -0.0063325767, 'text_feature_568': 0.056983214, 'text_feature_569': -0.009591145, 'text_feature_570': -0.014544207, 'text_feature_571': 0.040221952, 'text_feature_572': -0.010942638, 'text_feature_573': 0.030083828, 'text_feature_574': -0.012253747, 'text_feature_575': 0.067591585, 'text_feature_576': -0.014097692, 'text_feature_577': 0.0112438295, 'text_feature_578': 0.017029053, 'text_feature_579': 0.03749636, 'text_feature_580': -0.022279285, 'text_feature_581': -0.058446046, 'text_feature_582': 0.010721402, 'text_feature_583': 0.02511972, 'text_feature_584': 0.013906203, 'text_feature_585': 0.014673992, 'text_feature_586': -0.020979729, 'text_feature_587': 0.0001521369, 'text_feature_588': -0.021398071, 'text_feature_589': 0.053284418, 'text_feature_590': -0.02309256, 'text_feature_591': 0.052368194, 'text_feature_592': -0.066545315, 'text_feature_593': 0.0054422687, 'text_feature_594': -0.04268602, 'text_feature_595': -0.02798023, 'text_feature_596': -0.071015954, 'text_feature_597': -0.005811582, 'text_feature_598': 0.05467021, 'text_feature_599': -0.0134315835, 'text_feature_600': 0.04177412, 'text_feature_601': 0.03250346, 'text_feature_602': -0.005783086, 'text_feature_603': 0.08033364, 'text_feature_604': -0.031266443, 'text_feature_605': -0.028989755, 'text_feature_606': 0.013862792, 'text_feature_607': 0.015739404, 'text_feature_608': -0.038046263, 'text_feature_609': 0.028309094, 'text_feature_610': -0.0010382008, 'text_feature_611': 0.028527984, 'text_feature_612': 0.039058685, 'text_feature_613': -0.050765064, 'text_feature_614': -0.06471748, 'text_feature_615': -0.010626361, 'text_feature_616': -0.05161638, 'text_feature_617': -0.06942658, 'text_feature_618': -0.01325766, 'text_feature_619': -0.011138236, 'text_feature_620': 0.0126498435, 'text_feature_621': 0.0036897275, 'text_feature_622': -0.012114172, 'text_feature_623': 0.024381684, 'text_feature_624': 0.0041091735, 'text_feature_625': -0.012680513, 'text_feature_626': 0.02799094, 'text_feature_627': 0.023270424, 'text_feature_628': 0.053392198, 'text_feature_629': 0.033902824, 'text_feature_630': -0.015821826, 'text_feature_631': 0.060440276, 'text_feature_632': 0.001494693, 'text_feature_633': 0.012591629, 'text_feature_634': -0.08561639, 'text_feature_635': -0.009498354, 'text_feature_636': -0.006981292, 'text_feature_637': -0.01254667, 'text_feature_638': 0.013647827, 'text_feature_639': -0.0058711916, 'text_feature_640': -0.06054328, 'text_feature_641': 0.0009854416, 'text_feature_642': 0.015460302, 'text_feature_643': 0.024032975, 'text_feature_644': 0.02232813, 'text_feature_645': -0.015929813, 'text_feature_646': 0.02082112, 'text_feature_647': -0.0131252, 'text_feature_648': 0.014302971, 'text_feature_649': 0.03169819, 'text_feature_650': -0.005738214, 'text_feature_651': 0.0003537181, 'text_feature_652': 0.054047972, 'text_feature_653': -0.0054486096, 'text_feature_654': 0.03206763, 'text_feature_655': -0.03595696, 'text_feature_656': -0.036714748, 'text_feature_657': -0.011336024, 'text_feature_658': 0.022561343, 'text_feature_659': 0.0051836898, 'text_feature_660': 0.040722556, 'text_feature_661': 0.027861029, 'text_feature_662': -0.02695639, 'text_feature_663': -0.013236223, 'text_feature_664': 0.026726542, 'text_feature_665': 0.0154192215, 'text_feature_666': 0.040017884, 'text_feature_667': -0.03624818, 'text_feature_668': 0.037553295, 'text_feature_669': 0.033777334, 'text_feature_670': -0.030061936, 'text_feature_671': 0.0045258394, 'text_feature_672': -0.039820254, 'text_feature_673': 0.015236083, 'text_feature_674': 0.04999728, 'text_feature_675': 0.024606586, 'text_feature_676': -0.039720036, 'text_feature_677': 0.01918491, 'text_feature_678': -0.062150966, 'text_feature_679': -0.031934567, 'text_feature_680': 0.022822784, 'text_feature_681': -0.029283632, 'text_feature_682': -0.044032983, 'text_feature_683': 0.02928566, 'text_feature_684': -0.023096526, 'text_feature_685': 0.014658736, 'text_feature_686': 0.0015455072, 'text_feature_687': -0.011169629, 'text_feature_688': 0.009649921, 'text_feature_689': -0.0021805689, 'text_feature_690': -0.062881395, 'text_feature_691': 0.02074213, 'text_feature_692': -0.0086096255, 'text_feature_693': -0.002298367, 'text_feature_694': 0.015230329, 'text_feature_695': -0.01741249, 'text_feature_696': 0.07179538, 'text_feature_697': 0.029688895, 'text_feature_698': 0.0069733844, 'text_feature_699': -0.030665705, 'text_feature_700': 0.06763403, 'text_feature_701': 0.020973803, 'text_feature_702': 0.0024254618, 'text_feature_703': -0.04419377, 'text_feature_704': 0.0100390585, 'text_feature_705': -0.0051889634, 'text_feature_706': -0.011439502, 'text_feature_707': 0.02849917, 'text_feature_708': 0.054140653, 'text_feature_709': 0.046307676, 'text_feature_710': 0.014832008, 'text_feature_711': -0.019986516, 'text_feature_712': -0.010750922, 'text_feature_713': 0.05372096, 'text_feature_714': 0.009850198, 'text_feature_715': -0.00894916, 'text_feature_716': 0.015569805, 'text_feature_717': -0.0120609095, 'text_feature_718': -0.0029840693, 'text_feature_719': 0.052747406, 'text_feature_720': 0.0017044218, 'text_feature_721': -0.004359751, 'text_feature_722': 0.0009619323, 'text_feature_723': 0.03851872, 'text_feature_724': 0.0050286185, 'text_feature_725': 0.061098978, 'text_feature_726': -0.053994376, 'text_feature_727': -0.049397346, 'text_feature_728': -0.023956569, 'text_feature_729': -0.02380437, 'text_feature_730': 0.067149244, 'text_feature_731': 0.018358566, 'text_feature_732': -0.0025483635, 'text_feature_733': -0.01455332, 'text_feature_734': -0.022067212, 'text_feature_735': 0.056249965, 'text_feature_736': 0.07264776, 'text_feature_737': -0.01963201, 'text_feature_738': -0.03596707, 'text_feature_739': 8.85239e-05, 'text_feature_740': -0.015577528, 'text_feature_741': -0.016572775, 'text_feature_742': -0.0073142494, 'text_feature_743': -0.044916812, 'text_feature_744': -0.0029116024, 'text_feature_745': -0.0016663715, 'text_feature_746': 0.012913056, 'text_feature_747': -0.022955136, 'text_feature_748': -0.06876389, 'text_feature_749': 0.041100714, 'text_feature_750': 0.025367502, 'text_feature_751': -0.0785348, 'text_feature_752': 0.0003173212, 'text_feature_753': -0.061836883, 'text_feature_754': -0.030543158, 'text_feature_755': 0.034598447, 'text_feature_756': -0.05016737, 'text_feature_757': 0.04874558, 'text_feature_758': 0.0052600084, 'text_feature_759': 0.017960971, 'text_feature_760': 0.053402726, 'text_feature_761': -0.08196143, 'text_feature_762': 0.029364415, 'text_feature_763': -0.0004829505, 'text_feature_764': 0.05331725, 'text_feature_765': -0.030169707, 'text_feature_766': -0.0009237413, 'text_feature_767': 0.03949185, 'text_feature_768': -0.010876668, 'prediction_label': 0, 'prediction_score_0': 0.9990470409, 'prediction_score_1': 0.0009529882}]\n"
     ]
    }
   ],
   "source": [
    "# Combine predict and predict_proba in a DataFrame\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Predict labels and probabilities\n",
    "labels = xgb_model.predict(X_test_structured)\n",
    "proba = xgb_model.predict_proba(X_test_structured)\n",
    "\n",
    "# Create predictions DataFrame\n",
    "predictions = pd.DataFrame(proba, columns=[f\"prediction_score_{cls}\" for cls in xgb_model.classes_])\n",
    "predictions.insert(0, \"prediction_label\", labels)\n",
    "\n",
    "# Reset index if necessary (to ensure alignment during concat)\n",
    "X_test_vectorized_with_id = X_test_vectorized_with_id.reset_index(drop=True)\n",
    "predictions = predictions.reset_index(drop=True)\n",
    "\n",
    "# Combine features with predictions\n",
    "combined_df = pd.concat([X_test_vectorized_with_id, predictions], axis=1)\n",
    "\n",
    "# Convert to JSON (list of dicts)\n",
    "parsed_json = json.loads(combined_df.to_json(orient='records'))\n",
    "\n",
    "# Example output\n",
    "print(parsed_json[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81555a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_data(user_id, parsed_json, json_structures):\n",
    "    desired_data = next(item for item in parsed_json if item['id'] == int(user_id))\n",
    "\n",
    "    # get predicted_label from parsed_json\n",
    "    result = desired_data['prediction_label']\n",
    "    \n",
    "    shap_values = json_structures[int(user_id)]\n",
    "    \n",
    "    return result, shap_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c054c1",
   "metadata": {},
   "source": [
    "### 2.2 Prompt Template to generate narratives for SHAP explanations\n",
    "\n",
    "```markdown\n",
    "# System Prompt\n",
    "You are a churn explanation assistant. Provide clear, descriptive, business-oriented narratives that non-technical users can understand.\n",
    "\n",
    "# User Prompt\n",
    "The model predicted this customer will <CHURN_STATUS>.\n",
    "Top <TOP_N> features affecting the prediction:\n",
    "- <FEATURE_1>: <direction - in what direction it affects churn, either increase or decrease>, <magnitude - how strong the impact of the feature is> effect\n",
    "- <FEATURE_2>: <direction>, <magnitude> effect\n",
    "...\n",
    "- <FEATURE_N>: <direction>, <magnitude> effect\n",
    "\n",
    "Write a concise, business-friendly explanation for a non-technical user (50 words).  \n",
    "Describe why these features contribute to the prediction, using clear, empathetic language.  \n",
    "Do not use SHAP jargon or the phrase \"based on\".\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "For example:\n",
    "\n",
    "```markdown\n",
    "# System Prompt\n",
    "You are a churn explanation assistant. Provide clear, descriptive, business-oriented narratives that non-technical users can understand.\n",
    "\n",
    "# User Prompt\n",
    "The model predicted this customer will not churn.\n",
    "Top 5 features affecting the prediction:\n",
    "- Joined Through Referral Yes: decreases churn, very strong effect\n",
    "- Past Complaint Yes: increases churn, moderate effect\n",
    "- Membership Category: decreases churn, strong effect\n",
    "- Avg Frequency Login Days: decreases churn, strong effect\n",
    "- Points In Wallet: decreases churn, moderate effect\n",
    "\n",
    "Write a concise, business-friendly explanation for a non-technical user (50 words).  \n",
    "Describe why these features contribute to the prediction, using clear, empathetic language.  \n",
    "Do not use SHAP jargon or the phrase \"based on\".\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "369209d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, Any\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Feature magnitude mapping for SHAP values to reduce ambiguity\n",
    "def map_shap_magnitude(value):\n",
    "    abs_val = abs(value)\n",
    "    if abs_val > 5:\n",
    "        return \"very strong\"\n",
    "    elif abs_val > 1:\n",
    "        return \"strong\"\n",
    "    elif abs_val > 0.3:\n",
    "        return \"moderate\"\n",
    "    else:\n",
    "        return \"weak\"\n",
    "\n",
    "# Generate LLM-generated narratives for ML shap explanations\n",
    "def generate_churn_explainability(result: int, \n",
    "                                  shap_values: dict,\n",
    "                                  api_key: str,\n",
    "                                  top_n: int = 5, # Top 5 features are mentioned\n",
    "                                  model: str = \"gpt-4o-mini\",\n",
    "                                  temperature: float = 0.2,\n",
    "                                  show_prompt=False) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate churn explainability using OpenAI SDK directly\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize OpenAI client\n",
    "    client = openai.OpenAI(api_key=api_key)\n",
    "    \n",
    "    # Step 1: Select top features by absolute SHAP value\n",
    "    top_features = sorted(\n",
    "        shap_values.items(), key=lambda x: abs(x[1]), reverse=True\n",
    "    )[:top_n]\n",
    "\n",
    "    # Step 2: Preprocess features for LLM input\n",
    "    feature_list: list[str] = []\n",
    "    for feat, val in top_features:\n",
    "        direction = \"increases churn\" if val > 0 else \"decreases churn\"\n",
    "        magnitude = map_shap_magnitude(val)\n",
    "        description = feat.replace(\"_\", \" \").title()\n",
    "        feature_list.append(f\"- {description}: {direction}, {magnitude} effect\")\n",
    "\n",
    "    feature_text = \"\\n\".join(feature_list)\n",
    "\n",
    "    # Step 3: Create prompt for explanation\n",
    "    explanation_prompt = f\"\"\"\n",
    "The model predicted this customer will {'churn' if result == 1 else 'not churn'}.\n",
    "Top {top_n} features affecting the prediction:\n",
    "{feature_text}\n",
    "\n",
    "Write a concise, business-friendly explanation for a non-technical user (50 words).\n",
    "Describe why these features contribute to the prediction, using clear, empathetic language.\n",
    "Do not use SHAP jargon or the phrase \"based on\".\n",
    "\"\"\"\n",
    "    system_prompt = \"You are a churn explanation assistant. Provide clear, descriptive, business-oriented narratives that non-technical users can understand.\"\n",
    "    \n",
    "    if show_prompt is True:\n",
    "        print(\"PROMPT TEMPLATE FOR generate_churn_explainability function\")\n",
    "        print(\"===========\")\n",
    "        print(\"# System Prompt\")\n",
    "        print(system_prompt)\n",
    "        print(\"\\n# User Prompt\")\n",
    "        print(explanation_prompt)\n",
    "        print(\"===========\")\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": explanation_prompt}\n",
    "        ]\n",
    "    )\n",
    "    explanation = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Return results in the same format as the original function\n",
    "    return {\n",
    "        \"top_features\": feature_list,\n",
    "        'explanation': explanation\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a1171b",
   "metadata": {},
   "source": [
    "- Demo for the first row to `generate_churn_explainability`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85b52656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT TEMPLATE FOR generate_churn_explainability function\n",
      "===========\n",
      "# System Prompt\n",
      "You are a churn explanation assistant. Provide clear, descriptive, business-oriented narratives that non-technical users can understand.\n",
      "\n",
      "# User Prompt\n",
      "\n",
      "The model predicted this customer will not churn.\n",
      "Top 5 features affecting the prediction:\n",
      "- Membership Category: decreases churn, very strong effect\n",
      "- Avg Transaction Value: decreases churn, strong effect\n",
      "- Avg Frequency Login Days: decreases churn, moderate effect\n",
      "- Age: increases churn, weak effect\n",
      "- Avg Time Spent: decreases churn, weak effect\n",
      "\n",
      "Write a concise, business-friendly explanation for a non-technical user (50 words).\n",
      "Describe why these features contribute to the prediction, using clear, empathetic language.\n",
      "Do not use SHAP jargon or the phrase \"based on\".\n",
      "\n",
      "===========\n",
      "Demo: first row output\n",
      "{\n",
      "  \"top_features\": [\n",
      "    \"- Membership Category: decreases churn, very strong effect\",\n",
      "    \"- Avg Transaction Value: decreases churn, strong effect\",\n",
      "    \"- Avg Frequency Login Days: decreases churn, moderate effect\",\n",
      "    \"- Age: increases churn, weak effect\",\n",
      "    \"- Avg Time Spent: decreases churn, weak effect\"\n",
      "  ],\n",
      "  \"explanation\": \"The model suggests this customer is likely to stay. Their membership category and higher transaction values strongly encourage loyalty. Frequent logins also help, while age slightly increases the risk of leaving. Overall, their engagement and spending habits create a positive outlook for continued membership.\",\n",
      "  \"shap_values\": {\n",
      "    \"gender_M\": -0.027097957208752632,\n",
      "    \"region_category_Town\": -0.007706661708652973,\n",
      "    \"region_category_Village\": 0.006844460964202881,\n",
      "    \"joined_through_referral_Yes\": -0.12247615307569504,\n",
      "    \"preferred_offer_types_Gift Vouchers/Coupons\": -0.021200327202677727,\n",
      "    \"preferred_offer_types_Without Offers\": -0.02208494022488594,\n",
      "    \"medium_of_operation_Desktop\": -0.029541881754994392,\n",
      "    \"medium_of_operation_Smartphone\": -0.009845218621194363,\n",
      "    \"internet_option_Mobile_Data\": -0.03303796797990799,\n",
      "    \"internet_option_Wi-Fi\": -0.04274913668632507,\n",
      "    \"used_special_discount_Yes\": 0.08828579634428024,\n",
      "    \"offer_application_preference_Yes\": -0.07899238169193268,\n",
      "    \"past_complaint_Yes\": -0.010556424967944622,\n",
      "    \"years_since_joining\": 0.00038051384035497904,\n",
      "    \"membership_category\": -7.076250076293945,\n",
      "    \"complaint_status\": -0.01447039470076561,\n",
      "    \"age\": 0.22118932008743286,\n",
      "    \"days_since_last_login\": 0.007300875149667263,\n",
      "    \"avg_time_spent\": -0.1626385748386383,\n",
      "    \"avg_transaction_value\": -3.1553311347961426,\n",
      "    \"avg_frequency_login_days\": -0.8629294037818909,\n",
      "    \"points_in_wallet\": 0.10782947391271591\n",
      "  },\n",
      "  \"predicted_label\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Generate explanation for the first row only\n",
    "output_list = []\n",
    "\n",
    "# Get the first user_id\n",
    "first_user_id = X_test_sampled_with_id['id'].iloc[0]\n",
    "\n",
    "# Prepare input data\n",
    "result, shap_values = prepare_input_data(\n",
    "    first_user_id, parsed_json, json_structures\n",
    ")\n",
    "\n",
    "# Generate explanation\n",
    "output = generate_churn_explainability(\n",
    "    result=result,\n",
    "    shap_values=shap_values,\n",
    "    api_key=openai_api_key,\n",
    "    show_prompt=True\n",
    ")\n",
    "\n",
    "# Attach extra info\n",
    "output[\"shap_values\"] = shap_values\n",
    "output[\"predicted_label\"] = result\n",
    "\n",
    "# Store in list\n",
    "output_list.append(output)\n",
    "\n",
    "# Demo print\n",
    "print(\"Demo: first row output\")\n",
    "print(json.dumps(output, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a462c268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping to generate the LLM-generated narratives for SHAP explanations\n",
    "output_list = []\n",
    "\n",
    "# total number of samples\n",
    "n_samples = len(X_test_sampled_with_id)\n",
    "\n",
    "for idx, user_id in enumerate(X_test_sampled_with_id['id'], start=1):\n",
    "    result, shap_values = prepare_input_data(user_id, parsed_json, json_structures)\n",
    "\n",
    "    # Then generate explanations\n",
    "    output = generate_churn_explainability(\n",
    "        result=result,\n",
    "        shap_values=shap_values,\n",
    "        api_key=openai_api_key\n",
    "    )\n",
    "    \n",
    "    output[\"shap_values\"] = shap_values\n",
    "    output[\"predicted_label\"] = result\n",
    "    output_list.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3480f2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = os.makedirs(\"../../data/output\", exist_ok=True)\n",
    "\n",
    "shap_output_df = pd.DataFrame.from_records(output_list)\n",
    "shap_output_df.to_csv(\"../../data/output/llm_generated_narratives_on_shap.csv.gz\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b59c95c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top_features</th>\n",
       "      <th>explanation</th>\n",
       "      <th>shap_values</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[- Membership Category: decreases churn, very ...</td>\n",
       "      <td>The model suggests this customer is likely to ...</td>\n",
       "      <td>{'gender_M': -0.027097957208752632, 'region_ca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[- Membership Category: increases churn, stron...</td>\n",
       "      <td>The model indicates this customer may leave du...</td>\n",
       "      <td>{'gender_M': 0.02640422061085701, 'region_cate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[- Membership Category: increases churn, stron...</td>\n",
       "      <td>The model indicates this customer is likely to...</td>\n",
       "      <td>{'gender_M': 0.09955321252346039, 'region_cate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        top_features  \\\n",
       "0  [- Membership Category: decreases churn, very ...   \n",
       "1  [- Membership Category: increases churn, stron...   \n",
       "2  [- Membership Category: increases churn, stron...   \n",
       "\n",
       "                                         explanation  \\\n",
       "0  The model suggests this customer is likely to ...   \n",
       "1  The model indicates this customer may leave du...   \n",
       "2  The model indicates this customer is likely to...   \n",
       "\n",
       "                                         shap_values  predicted_label  \n",
       "0  {'gender_M': -0.027097957208752632, 'region_ca...                0  \n",
       "1  {'gender_M': 0.02640422061085701, 'region_cate...                1  \n",
       "2  {'gender_M': 0.09955321252346039, 'region_cate...                1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shap_output_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15a15881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 4)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double check for the dimension of the shap_output_df\n",
    "shap_output_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ad5497c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predicted_label\n",
       "1    16\n",
       "0    14\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure both target variable having equal distribution\n",
    "shap_output_df[\"predicted_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1760674a",
   "metadata": {},
   "source": [
    "## Part 3: LLM As A Judge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbbee34",
   "metadata": {},
   "source": [
    "### Prompt Template for LLM As A Judge\n",
    "\n",
    "```markdown\n",
    "# System Prompt\n",
    "You are an impartial judge and an expert in machine learning explainability.  \n",
    "Your task is to evaluate the alignment between a generated churn narrative and the provided SHAP values.\n",
    "\n",
    "You will receive:\n",
    "- The model prediction result\n",
    "- The customer's profile description\n",
    "- The top 5 SHAP values\n",
    "- A generated narrative to evaluate\n",
    "\n",
    "Your job is to check which features in the narrative align with the SHAP values in terms of **direction** (towards churn or away from churn) and **magnitude** (strong, weak, etc.).\n",
    "\n",
    "# User Prompt\n",
    "**[Context Data]**\n",
    "- Predicted Churn Result: <PREDICTED_LABEL> (0 = will not churn, 1 = will churn)\n",
    "- Top 5 SHAP Values: <TOP_5_SHAP_VALUES>\n",
    "\n",
    "**[Generated Narrative to Evaluate]**\n",
    "<GENERATED_NARRATIVE>\n",
    "\n",
    "Please return your evaluation in this format:\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"feature\": \"<feature_name>\",\n",
    "    \"mentioned_in_text\": <True|False>,\n",
    "    \"direction_text\": \"<direction extracted from text_direction>\",\n",
    "    \"direction_shap\": \"<direction extracted from shap_direction_magnitude>\",\n",
    "    \"magnitude_shap\": \"<magnitude extracted from shap_direction_magnitude>\",\n",
    "    \"direction_match\": <True|False>,\n",
    "    \"magnitude_match\": <True|False>\n",
    "  },\n",
    "  ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1aacb200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "prepare_for_judge_df = pd.read_csv('../../data/output/llm_generated_narratives_on_shap.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1bb04df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top_features</th>\n",
       "      <th>explanation</th>\n",
       "      <th>shap_values</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['- Membership Category: decreases churn, very...</td>\n",
       "      <td>The model suggests this customer is likely to ...</td>\n",
       "      <td>{'gender_M': -0.027097957208752632, 'region_ca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['- Membership Category: increases churn, stro...</td>\n",
       "      <td>The model indicates this customer may leave du...</td>\n",
       "      <td>{'gender_M': 0.02640422061085701, 'region_cate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['- Membership Category: increases churn, stro...</td>\n",
       "      <td>The model indicates this customer is likely to...</td>\n",
       "      <td>{'gender_M': 0.09955321252346039, 'region_cate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        top_features  \\\n",
       "0  ['- Membership Category: decreases churn, very...   \n",
       "1  ['- Membership Category: increases churn, stro...   \n",
       "2  ['- Membership Category: increases churn, stro...   \n",
       "\n",
       "                                         explanation  \\\n",
       "0  The model suggests this customer is likely to ...   \n",
       "1  The model indicates this customer may leave du...   \n",
       "2  The model indicates this customer is likely to...   \n",
       "\n",
       "                                         shap_values  predicted_label  \n",
       "0  {'gender_M': -0.027097957208752632, 'region_ca...                0  \n",
       "1  {'gender_M': 0.02640422061085701, 'region_cate...                1  \n",
       "2  {'gender_M': 0.09955321252346039, 'region_cate...                1  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_for_judge_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4562dce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top 5 most important features from SHAP values\n",
    "def get_top_shap_values_dict(shap_str, top_n=5):\n",
    "    # Convert string to dict if needed\n",
    "    if isinstance(shap_str, str):\n",
    "        shap_dict = ast.literal_eval(shap_str)\n",
    "    else:\n",
    "        shap_dict = shap_str\n",
    "    \n",
    "    # Sort by absolute values (largest to smallest), but keep original values\n",
    "    sorted_items = sorted(shap_dict.items(), key=lambda x: abs(x[1]), reverse=True)[:top_n]\n",
    "    \n",
    "    # Create dictionary with feature names and values\n",
    "    top_shap_dict = {}\n",
    "    for feature_name, value in sorted_items:\n",
    "        top_shap_dict[feature_name] = value\n",
    "    \n",
    "    return top_shap_dict\n",
    "\n",
    "prepare_for_judge_df['top_5_shap_values'] = prepare_for_judge_df['shap_values'].apply(\n",
    "        lambda x: get_top_shap_values_dict(x, top_n=5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1aafbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "def evaluate_narrative_with_llm(explanation_text, top_5_shap_values, predicted_label, show_prompt=False):\n",
    "    \"\"\"\n",
    "    Use OpenAI LLM as a judge to evaluate narrative quality feature-by-feature\n",
    "    \"\"\"\n",
    "    \n",
    "    system_prompt = f\"\"\"You are an impartial judge and an expert in machine learning explainability. \n",
    "Your task is to evaluate the alignment between a generated churn narrative and the provided SHAP values.\n",
    "\n",
    "You will receive:\n",
    "- The model prediction result\n",
    "- The customer's profile description\n",
    "- The top 5 SHAP values\n",
    "- A generated narrative to evaluate\n",
    "\n",
    "Your job is to check which features in the narrative align with the SHAP values in terms of **direction** \n",
    "(towards churn or away from churn) and **magnitude** (strong, weak, etc.).\n",
    "\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "**[Context Data]**\n",
    "- Predicted Churn Result: {predicted_label} (0 = will not churn, 1 = will churn)\n",
    "- Top 5 SHAP Values: {top_5_shap_values}\n",
    "\n",
    "**[Generated Narrative to Evaluate]**\n",
    "{explanation_text}\n",
    "\n",
    "Please return your evaluation in this format:\n",
    "\n",
    "```json\n",
    "[\n",
    "  {{\n",
    "    \"feature\": \"<feature_name>\",\n",
    "    \"mentioned_in_text\": <True|False>,\n",
    "    \"direction_text\": \"<direction extracted from text_direction>\",\n",
    "    \"direction_shap\": \"<direction extracted from shap_direction_magnitude>\",\n",
    "    \"magnitude_shap\": \"<magnitude extracted from shap_direction_magnitude>\",\n",
    "    \"direction_match\": <True|False>,\n",
    "    \"magnitude_match\": <True|False>\n",
    "  }},\n",
    "  ...\n",
    "]\n",
    "```\n",
    "\"\"\"\n",
    "    # Initialize OpenAI client\n",
    "    client = OpenAI(api_key=openai_api_key)\n",
    "    \n",
    "    if show_prompt is True:\n",
    "        print(\"PROMPT TEMPLATE FOR evaluate_narrative_with_llm function\")\n",
    "        print(\"===========\")\n",
    "        print(\"# System Prompt\")\n",
    "        print(system_prompt)\n",
    "        print(\"\\n# User Prompt\")\n",
    "        print(user_prompt)\n",
    "        print(\"===========\")\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            max_tokens=1000\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling OpenAI API: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13b713fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating first explanation only...\n",
      "PROMPT TEMPLATE FOR evaluate_narrative_with_llm function\n",
      "===========\n",
      "# System Prompt\n",
      "You are an impartial judge and an expert in machine learning explainability. \n",
      "Your task is to evaluate the alignment between a generated churn narrative and the provided SHAP values.\n",
      "\n",
      "You will receive:\n",
      "- The model prediction result\n",
      "- The customer's profile description\n",
      "- The top 5 SHAP values\n",
      "- A generated narrative to evaluate\n",
      "\n",
      "Your job is to check which features in the narrative align with the SHAP values in terms of **direction** \n",
      "(towards churn or away from churn) and **magnitude** (strong, weak, etc.).\n",
      "\n",
      "\n",
      "# User Prompt\n",
      "\n",
      "**[Context Data]**\n",
      "- Predicted Churn Result: 0 (0 = will not churn, 1 = will churn)\n",
      "- Top 5 SHAP Values: {'membership_category': -7.076250076293945, 'avg_transaction_value': -3.1553311347961426, 'avg_frequency_login_days': -0.8629294037818909, 'age': 0.22118932008743286, 'avg_time_spent': -0.1626385748386383}\n",
      "\n",
      "**[Generated Narrative to Evaluate]**\n",
      "The model suggests this customer is likely to stay. Their membership category and higher transaction values strongly encourage loyalty. Frequent logins and time spent on our platform also help, while age has a minor negative impact. Overall, these factors indicate a positive engagement with our services.\n",
      "\n",
      "Please return your evaluation in this format:\n",
      "\n",
      "```json\n",
      "[\n",
      "  {\n",
      "    \"feature\": \"<feature_name>\",\n",
      "    \"mentioned_in_text\": <True|False>,\n",
      "    \"direction_text\": \"<direction extracted from text_direction>\",\n",
      "    \"direction_shap\": \"<direction extracted from shap_direction_magnitude>\",\n",
      "    \"magnitude_shap\": \"<magnitude extracted from shap_direction_magnitude>\",\n",
      "    \"direction_match\": <True|False>,\n",
      "    \"magnitude_match\": <True|False>\n",
      "  },\n",
      "  ...\n",
      "]\n",
      "```\n",
      "\n",
      "===========\n",
      "\n",
      "Demo:\n",
      "=========\n",
      "\n",
      "Predicted Label:  0\n",
      "\n",
      "Explanation:  The model suggests this customer is likely to stay. Their membership category and higher transaction values strongly encourage loyalty. Frequent logins and time spent on our platform also help, while age has a minor negative impact. Overall, these factors indicate a positive engagement with our services.\n",
      "\n",
      "Top 5 SHAP magnitudes:  {'membership_category': 'very strong', 'avg_transaction_value': 'strong', 'avg_frequency_login_days': 'moderate', 'age': 'weak', 'avg_time_spent': 'weak'}\n",
      "\n",
      "Top 5 SHAP values:  {'membership_category': -7.076250076293945, 'avg_transaction_value': -3.1553311347961426, 'avg_frequency_login_days': -0.8629294037818909, 'age': 0.22118932008743286, 'avg_time_spent': -0.1626385748386383}\n",
      "\n",
      "Evaluation:  {\n",
      "  \"completeness\": {\n",
      "    \"score\": 10.0\n",
      "  },\n",
      "  \"accuracy\": {\n",
      "    \"score\": 10.0,\n",
      "    \"direction_points\": 5,\n",
      "    \"magnitude_points\": 5\n",
      "  },\n",
      "  \"raw_features\": [\n",
      "    {\n",
      "      \"feature\": \"membership_category\",\n",
      "      \"mentioned_in_text\": true,\n",
      "      \"direction_text\": \"encourage loyalty\",\n",
      "      \"direction_shap\": \"away from churn\",\n",
      "      \"magnitude_shap\": \"strong\",\n",
      "      \"direction_match\": true,\n",
      "      \"magnitude_match\": true\n",
      "    },\n",
      "    {\n",
      "      \"feature\": \"avg_transaction_value\",\n",
      "      \"mentioned_in_text\": true,\n",
      "      \"direction_text\": \"encourage loyalty\",\n",
      "      \"direction_shap\": \"away from churn\",\n",
      "      \"magnitude_shap\": \"strong\",\n",
      "      \"direction_match\": true,\n",
      "      \"magnitude_match\": true\n",
      "    },\n",
      "    {\n",
      "      \"feature\": \"avg_frequency_login_days\",\n",
      "      \"mentioned_in_text\": true,\n",
      "      \"direction_text\": \"help\",\n",
      "      \"direction_shap\": \"away from churn\",\n",
      "      \"magnitude_shap\": \"weak\",\n",
      "      \"direction_match\": true,\n",
      "      \"magnitude_match\": true\n",
      "    },\n",
      "    {\n",
      "      \"feature\": \"age\",\n",
      "      \"mentioned_in_text\": true,\n",
      "      \"direction_text\": \"negative impact\",\n",
      "      \"direction_shap\": \"towards churn\",\n",
      "      \"magnitude_shap\": \"weak\",\n",
      "      \"direction_match\": true,\n",
      "      \"magnitude_match\": true\n",
      "    },\n",
      "    {\n",
      "      \"feature\": \"avg_time_spent\",\n",
      "      \"mentioned_in_text\": true,\n",
      "      \"direction_text\": \"help\",\n",
      "      \"direction_shap\": \"away from churn\",\n",
      "      \"magnitude_shap\": \"weak\",\n",
      "      \"direction_match\": true,\n",
      "      \"magnitude_match\": true\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Just evaluate the first explanation\n",
    "import re\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"Evaluating first explanation only...\")\n",
    "\n",
    "first_row = prepare_for_judge_df.dropna(subset=['explanation']).iloc[0]\n",
    "\n",
    "evaluation = evaluate_narrative_with_llm(\n",
    "    first_row['explanation'],\n",
    "    first_row['top_5_shap_values'],\n",
    "    first_row['predicted_label'],\n",
    "    show_prompt=True\n",
    ")\n",
    "\n",
    "if evaluation:\n",
    "    try:\n",
    "        # Extract JSON inside ```json ... ```\n",
    "        match = re.search(r\"```json\\s*(\\[.*?\\])\\s*```\", evaluation, re.DOTALL)\n",
    "        if match:\n",
    "            clean_eval = match.group(1)\n",
    "        else:\n",
    "            clean_eval = evaluation.strip()\n",
    "\n",
    "        feature_data = json.loads(clean_eval)  # list of per-feature dicts\n",
    "\n",
    "        # ---- Compute factual accuracy score (0–10) ----\n",
    "        direction_points = sum(1 for f in feature_data if f.get(\"direction_match\"))\n",
    "        magnitude_points = sum(1 for f in feature_data if f.get(\"magnitude_match\"))\n",
    "        correct_points = direction_points + magnitude_points\n",
    "        max_points = 2 * len(feature_data)\n",
    "        accuracy_score = (correct_points / max_points) * 10 if max_points > 0 else 0\n",
    "\n",
    "        # ---- Scale completeness (from LLM 1–5 → 1–10) ----\n",
    "        # If you already collect LLM completeness score in 1–5\n",
    "        # ---- Calculate completeness based on mentioned features ----\n",
    "        mentioned_features = sum(1 for f in feature_data if f.get(\"mentioned_in_text\"))\n",
    "        completeness_raw = mentioned_features  # Count of features mentioned in text (0-5)\n",
    "        completeness_score_10 = round((completeness_raw / 5) * 10, 1)  # Scale to 0-10\n",
    "\n",
    "        # ---- Build eval_data summary ----\n",
    "        eval_data = {\n",
    "            \"completeness\": {\"score\": completeness_score_10},\n",
    "            \"accuracy\": {\n",
    "                \"score\": round(accuracy_score, 1),\n",
    "                \"direction_points\": direction_points,\n",
    "                \"magnitude_points\": magnitude_points\n",
    "            },\n",
    "            \"raw_features\": feature_data\n",
    "        }\n",
    "\n",
    "        results.append({\n",
    "            'index': first_row.name,\n",
    "            'explanation': first_row['explanation'],\n",
    "            'predicted_label': first_row['predicted_label'],\n",
    "            \"top_5_shap_magnitudes\": {feature: map_shap_magnitude(value) for feature, value in first_row['top_5_shap_values'].items()},\n",
    "            \"top_5_shap_values\": first_row['top_5_shap_values'],\n",
    "            'evaluation': eval_data\n",
    "        })\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed to parse JSON for the first row, storing raw output.\")\n",
    "        results.append({\n",
    "            'index': first_row.name,\n",
    "            'evaluation': evaluation\n",
    "        })\n",
    "\n",
    "# Save results\n",
    "evaluation_df = pd.DataFrame(results)\n",
    "print(\"\\nDemo:\\n=========\")\n",
    "print(\"\\nPredicted Label: \", evaluation_df[\"predicted_label\"][0])\n",
    "print(\"\\nExplanation: \", evaluation_df[\"explanation\"][0])\n",
    "print(\"\\nTop 5 SHAP magnitudes: \", evaluation_df[\"top_5_shap_magnitudes\"][0])\n",
    "print(\"\\nTop 5 SHAP values: \", evaluation_df[\"top_5_shap_values\"][0])\n",
    "print(\"\\nEvaluation: \", json.dumps(evaluation_df[\"evaluation\"][0], indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b3d43a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 30 explanations...\n",
      "Evaluating explanation 1/30\n",
      "Evaluating explanation 2/30\n",
      "Evaluating explanation 3/30\n",
      "Evaluating explanation 4/30\n",
      "Evaluating explanation 5/30\n",
      "Evaluating explanation 6/30\n",
      "Evaluating explanation 7/30\n",
      "Evaluating explanation 8/30\n",
      "Evaluating explanation 9/30\n",
      "Evaluating explanation 10/30\n",
      "Evaluating explanation 11/30\n",
      "Evaluating explanation 12/30\n",
      "Evaluating explanation 13/30\n",
      "Evaluating explanation 14/30\n",
      "Evaluating explanation 15/30\n",
      "Evaluating explanation 16/30\n",
      "Evaluating explanation 17/30\n",
      "Evaluating explanation 18/30\n",
      "Evaluating explanation 19/30\n",
      "Evaluating explanation 20/30\n",
      "Evaluating explanation 21/30\n",
      "Evaluating explanation 22/30\n",
      "Evaluating explanation 23/30\n",
      "Evaluating explanation 24/30\n",
      "Evaluating explanation 25/30\n",
      "Evaluating explanation 26/30\n",
      "Evaluating explanation 27/30\n",
      "Evaluating explanation 28/30\n",
      "Evaluating explanation 29/30\n",
      "Evaluating explanation 30/30\n"
     ]
    }
   ],
   "source": [
    "# Just evaluate all explanationsmp\n",
    "import re\n",
    "\n",
    "results = []\n",
    "\n",
    "print(f\"Evaluating {len(prepare_for_judge_df)} explanations...\")\n",
    "\n",
    "for idx, row in prepare_for_judge_df.iterrows():\n",
    "    if pd.isna(row['explanation']):\n",
    "        continue\n",
    "    \n",
    "    print(f\"Evaluating explanation {idx+1}/{len(prepare_for_judge_df)}\")\n",
    "    \n",
    "    evaluation = evaluate_narrative_with_llm(\n",
    "        row['explanation'],\n",
    "        row['top_5_shap_values'],\n",
    "        row['predicted_label']\n",
    "    )\n",
    "\n",
    "    if evaluation:\n",
    "        try:\n",
    "            # Extract JSON inside ```json ... ```\n",
    "            match = re.search(r\"```json\\s*(\\[.*?\\])\\s*```\", evaluation, re.DOTALL)\n",
    "            if match:\n",
    "                clean_eval = match.group(1)\n",
    "            else:\n",
    "                clean_eval = evaluation.strip()\n",
    "\n",
    "            feature_data = json.loads(clean_eval)  # list of per-feature dicts\n",
    "\n",
    "            # ---- Compute factual accuracy score (0–10) ----\n",
    "            direction_points = sum(1 for f in feature_data if f.get(\"direction_match\"))\n",
    "            magnitude_points = sum(1 for f in feature_data if f.get(\"magnitude_match\"))\n",
    "            correct_points = direction_points + magnitude_points\n",
    "            max_points = 2 * len(feature_data)\n",
    "            accuracy_score = (correct_points / max_points) * 10 if max_points > 0 else 0\n",
    "\n",
    "            # ---- Scale completeness (from LLM 1–5 → 1–10) ----\n",
    "            mentioned_features = sum(1 for f in feature_data if f.get(\"mentioned_in_text\"))\n",
    "            completeness_raw = mentioned_features  # Count of features mentioned in text (0-5)\n",
    "            completeness_score_10 = round((completeness_raw / 5) * 10, 1)  # Scale to 0-10\n",
    "\n",
    "            # ---- Build eval_data summary ----\n",
    "            eval_data = {\n",
    "                \"completeness\": {\"score\": completeness_score_10},\n",
    "                \"accuracy\": {\n",
    "                    \"score\": round(accuracy_score, 1),\n",
    "                    \"direction_points\": direction_points,\n",
    "                    \"magnitude_points\": magnitude_points\n",
    "                },\n",
    "                \"raw_features\": feature_data\n",
    "            }\n",
    "\n",
    "            results.append({\n",
    "                'index': idx,\n",
    "                'explanation': row['explanation'],\n",
    "                'predicted_label': row['predicted_label'],\n",
    "                \"top_5_shap_magnitudes\": {feature: map_shap_magnitude(value) for feature, value in row['top_5_shap_values'].items()},\n",
    "                \"top_5_shap_values\": row['top_5_shap_values'],\n",
    "                'evaluation': eval_data\n",
    "            })\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Failed to parse JSON for row {idx}, storing raw output.\")\n",
    "            results.append({\n",
    "                'index': idx,\n",
    "                'evaluation': evaluation\n",
    "            })\n",
    "\n",
    "# Save results\n",
    "evaluation_df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48c2b469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to 'data/output/llm_judge_evaluation_results.csv.gz'\n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "evaluation_df = pd.DataFrame(results)\n",
    "evaluation_df.to_csv('../../data/output/llm_judge_evaluation_results.csv.gz', index=False)\n",
    "\n",
    "print(\"Results saved to 'data/output/llm_judge_evaluation_results.csv.gz'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b58523d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 6)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double check the dimension of evaluation_df\n",
    "evaluation_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b125bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>explanation</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>top_5_shap_magnitudes</th>\n",
       "      <th>top_5_shap_values</th>\n",
       "      <th>evaluation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The model suggests this customer is likely to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'membership_category': 'very strong', 'avg_tr...</td>\n",
       "      <td>{'membership_category': -7.076250076293945, 'a...</td>\n",
       "      <td>{'completeness': {'score': 10.0}, 'accuracy': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The model indicates this customer may leave du...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'membership_category': 'strong', 'avg_frequen...</td>\n",
       "      <td>{'membership_category': 2.8404221534729004, 'a...</td>\n",
       "      <td>{'completeness': {'score': 10.0}, 'accuracy': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The model indicates this customer is likely to...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'membership_category': 'strong', 'avg_frequen...</td>\n",
       "      <td>{'membership_category': 2.958069324493408, 'av...</td>\n",
       "      <td>{'completeness': {'score': 10.0}, 'accuracy': ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                        explanation  predicted_label  \\\n",
       "0      0  The model suggests this customer is likely to ...                0   \n",
       "1      1  The model indicates this customer may leave du...                1   \n",
       "2      2  The model indicates this customer is likely to...                1   \n",
       "\n",
       "                               top_5_shap_magnitudes  \\\n",
       "0  {'membership_category': 'very strong', 'avg_tr...   \n",
       "1  {'membership_category': 'strong', 'avg_frequen...   \n",
       "2  {'membership_category': 'strong', 'avg_frequen...   \n",
       "\n",
       "                                   top_5_shap_values  \\\n",
       "0  {'membership_category': -7.076250076293945, 'a...   \n",
       "1  {'membership_category': 2.8404221534729004, 'a...   \n",
       "2  {'membership_category': 2.958069324493408, 'av...   \n",
       "\n",
       "                                          evaluation  \n",
       "0  {'completeness': {'score': 10.0}, 'accuracy': ...  \n",
       "1  {'completeness': {'score': 10.0}, 'accuracy': ...  \n",
       "2  {'completeness': {'score': 10.0}, 'accuracy': ...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ff3dc0",
   "metadata": {},
   "source": [
    "## Part 4: Evaluation Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "388d9778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Scores for Each Row:\n",
      "==================================================\n",
      "Row 1: Accuracy = 10.0, Completeness = 10.0\n",
      "Row 2: Accuracy = 10.0, Completeness = 10.0\n",
      "Row 3: Accuracy = 10.0, Completeness = 10.0\n",
      "Row 4: Accuracy = 8.0, Completeness = 8.0\n",
      "Row 5: Accuracy = 8.0, Completeness = 8.0\n",
      "Row 6: Accuracy = 10.0, Completeness = 10.0\n",
      "Row 7: Accuracy = 7.0, Completeness = 8.0\n",
      "Row 8: Accuracy = 9.0, Completeness = 10.0\n",
      "Row 9: Accuracy = 10.0, Completeness = 10.0\n",
      "Row 10: Accuracy = 10.0, Completeness = 10.0\n",
      "Row 11: Accuracy = 10.0, Completeness = 10.0\n",
      "Row 12: Accuracy = 8.0, Completeness = 10.0\n",
      "Row 13: Accuracy = 9.0, Completeness = 10.0\n",
      "Row 14: Accuracy = 10.0, Completeness = 10.0\n",
      "Row 15: Accuracy = 10.0, Completeness = 10.0\n",
      "Row 16: Accuracy = 10.0, Completeness = 10.0\n",
      "Row 17: Accuracy = 9.0, Completeness = 10.0\n",
      "Row 18: Accuracy = 10.0, Completeness = 10.0\n",
      "Row 19: Accuracy = 9.0, Completeness = 10.0\n",
      "Row 20: Accuracy = 10.0, Completeness = 10.0\n",
      "Row 21: Accuracy = 8.0, Completeness = 8.0\n",
      "Row 22: Accuracy = 8.0, Completeness = 8.0\n",
      "Row 23: Accuracy = 8.0, Completeness = 8.0\n",
      "Row 24: Accuracy = 10.0, Completeness = 10.0\n",
      "Row 25: Accuracy = 8.0, Completeness = 8.0\n",
      "Row 26: Accuracy = 7.0, Completeness = 10.0\n",
      "Row 27: Accuracy = 8.0, Completeness = 8.0\n",
      "Row 28: Accuracy = 4.0, Completeness = 8.0\n",
      "Row 29: Accuracy = 10.0, Completeness = 10.0\n",
      "Row 30: Accuracy = 8.0, Completeness = 10.0\n",
      "\n",
      "==================================================\n",
      "Summary Statistics:\n",
      "Average Accuracy Score: 8.87\n",
      "Average Completeness Score: 9.40\n",
      "Accuracy Score Range: 4.0 - 10.0\n",
      "Completeness Score Range: 8.0 - 10.0\n",
      "\n",
      "Detailed Scores DataFrame:\n",
      "    Row  Accuracy_Score  Completeness_Score  Predicted_Label\n",
      "0     1            10.0                10.0                0\n",
      "1     2            10.0                10.0                1\n",
      "2     3            10.0                10.0                1\n",
      "3     4             8.0                 8.0                0\n",
      "4     5             8.0                 8.0                1\n",
      "5     6            10.0                10.0                0\n",
      "6     7             7.0                 8.0                0\n",
      "7     8             9.0                10.0                0\n",
      "8     9            10.0                10.0                1\n",
      "9    10            10.0                10.0                1\n",
      "10   11            10.0                10.0                1\n",
      "11   12             8.0                10.0                1\n",
      "12   13             9.0                10.0                1\n",
      "13   14            10.0                10.0                0\n",
      "14   15            10.0                10.0                1\n",
      "15   16            10.0                10.0                0\n",
      "16   17             9.0                10.0                0\n",
      "17   18            10.0                10.0                0\n",
      "18   19             9.0                10.0                1\n",
      "19   20            10.0                10.0                0\n",
      "20   21             8.0                 8.0                0\n",
      "21   22             8.0                 8.0                0\n",
      "22   23             8.0                 8.0                1\n",
      "23   24            10.0                10.0                0\n",
      "24   25             8.0                 8.0                0\n",
      "25   26             7.0                10.0                1\n",
      "26   27             8.0                 8.0                1\n",
      "27   28             4.0                 8.0                1\n",
      "28   29            10.0                10.0                1\n",
      "29   30             8.0                10.0                1\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "# Convert string representations of dicts to actual dicts if needed\n",
    "evaluation_df['evaluation'] = evaluation_df['evaluation'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Extract individual scores for each row\n",
    "accuracy_scores = evaluation_df['evaluation'].apply(lambda x: x['accuracy']['score']).values\n",
    "completeness_scores = evaluation_df['evaluation'].apply(lambda x: x['completeness']['score']).values\n",
    "\n",
    "# Print individual scores for each row\n",
    "print(\"Individual Scores for Each Row:\")\n",
    "print(\"=\" * 50)\n",
    "for idx, (acc, comp) in enumerate(zip(accuracy_scores, completeness_scores)):\n",
    "    print(f\"Row {idx+1}: Accuracy = {acc}, Completeness = {comp}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Summary Statistics:\")\n",
    "print(f\"Average Accuracy Score: {accuracy_scores.mean():.2f}\")\n",
    "print(f\"Average Completeness Score: {completeness_scores.mean():.2f}\")\n",
    "print(f\"Accuracy Score Range: {accuracy_scores.min():.1f} - {accuracy_scores.max():.1f}\")\n",
    "print(f\"Completeness Score Range: {completeness_scores.min():.1f} - {completeness_scores.max():.1f}\")\n",
    "\n",
    "# Optional: Create a DataFrame for easier viewing\n",
    "scores_df = pd.DataFrame({\n",
    "    'Row': range(1, len(accuracy_scores) + 1),\n",
    "    'Accuracy_Score': accuracy_scores,\n",
    "    'Completeness_Score': completeness_scores,\n",
    "    'Predicted_Label': evaluation_df['predicted_label'].values\n",
    "})\n",
    "\n",
    "print(\"\\nDetailed Scores DataFrame:\")\n",
    "print(scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9665ba71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "KEY STATISTICAL ANALYSIS\n",
      "============================================================\n",
      "\n",
      "1. SCORE DISTRIBUTIONS:\n",
      "----------------------------------------\n",
      "Accuracy Score Distribution:\n",
      "4.0      1\n",
      "7.0      2\n",
      "8.0      9\n",
      "9.0      4\n",
      "10.0    14\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Completeness Score Distribution:\n",
      "8.0      9\n",
      "10.0    21\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Key statistical analysis\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY STATISTICAL ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Score distributions\n",
    "print(\"\\n1. SCORE DISTRIBUTIONS:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"Accuracy Score Distribution:\")\n",
    "accuracy_counts = pd.Series(accuracy_scores).value_counts().sort_index()\n",
    "print(accuracy_counts)\n",
    "\n",
    "print(\"\\nCompleteness Score Distribution:\")\n",
    "completeness_counts = pd.Series(completeness_scores).value_counts().sort_index()\n",
    "print(completeness_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ae761a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. PERFORMANCE BY PREDICTED LABEL:\n",
      "----------------------------------------\n",
      "                Accuracy_Score       Completeness_Score      \n",
      "                          mean count               mean count\n",
      "Predicted_Label                                              \n",
      "0                         9.07    14               9.29    14\n",
      "1                         8.69    16               9.50    16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Performance by predicted label (most important)\n",
    "print(\"\\n2. PERFORMANCE BY PREDICTED LABEL:\")\n",
    "print(\"-\" * 40)\n",
    "label_analysis = pd.DataFrame({\n",
    "    'Predicted_Label': evaluation_df['predicted_label'].values,\n",
    "    'Accuracy_Score': accuracy_scores,\n",
    "    'Completeness_Score': completeness_scores\n",
    "})\n",
    "\n",
    "label_stats = label_analysis.groupby('Predicted_Label').agg({\n",
    "    'Accuracy_Score': ['mean', 'count'],\n",
    "    'Completeness_Score': ['mean', 'count']\n",
    "}).round(2)\n",
    "print(label_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7d5a9568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. HIGH-QUALITY EXPLANATIONS:\n",
      "----------------------------------------\n",
      "High accuracy (≥7): 29/30 (96.7%)\n",
      "High completeness (≥7): 30/30 (100.0%)\n",
      "Both high (≥7): 29/30 (96.7%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. High-quality explanations count (business-relevant)\n",
    "print(\"\\n3. HIGH-QUALITY EXPLANATIONS:\")\n",
    "print(\"-\" * 40)\n",
    "high_accuracy = (accuracy_scores >= 7).sum()\n",
    "high_completeness = (completeness_scores >= 7).sum()\n",
    "both_high = ((accuracy_scores >= 7) & (completeness_scores >= 7)).sum()\n",
    "\n",
    "print(f\"High accuracy (≥7): {high_accuracy}/{len(accuracy_scores)} ({high_accuracy/len(accuracy_scores)*100:.1f}%)\")\n",
    "print(f\"High completeness (≥7): {high_completeness}/{len(completeness_scores)} ({high_completeness/len(completeness_scores)*100:.1f}%)\")\n",
    "print(f\"Both high (≥7): {both_high}/{len(accuracy_scores)} ({both_high/len(accuracy_scores)*100:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
